---
title: "Rasch-model"
author: "Merlin"
date: "2024-10-09"
output: html_document
---
The data simulation
```{r}
# Data simulation as provided
set.seed(123)
beta_true <- rnorm(10, 0, 1) # True item difficulty 
theta_true <- rnorm(100, 0.5 , 1) #True latent trait 

# Probability of correct response for each item
simdata <- function(beta, theta){ # Insert the beta and theta values you have
  
  p <- matrix(0, nrow = length(theta), ncol = length(beta)) # setup matrix
  answer <- matrix(0, nrow = length(theta), ncol = length(beta)) # setup matrix
  
  for (i in 1:length(beta)){ #repeat for every beta_i
    
    p[,i] <- exp(theta - beta[i]) / (1 + exp(theta - beta[i])) # Probability of correct answer
    answer[,i] <- rbinom(length(theta), 1, p[,i]) # 0s and 1s whether the answer was correct 
  
  }
  
  return(list(p = p, answer = answer)) # Give a list including the probability matrix and the answer (0, 1) matrix
  
}

responses <- simdata(beta_true, theta_true) # run function for our given true item difficulty and latent trait, store it in responses
response_matrix <- responses$answer # store only the answers (0, 1) in response matrix 
```


The estimation
```{r}
# Required package for matrix operations
library(MASS)  # For the ginv() function (generalized inverse)

# Define the log-likelihood for all beta values using the sum algorithm
logLik_rasch <- function(beta, response_matrix) { s
  N <- nrow(response_matrix)  # Number of persons
  J <- ncol(response_matrix)  # Number of items
  
  S_i <- rowSums(response_matrix)  # Total correct responses per person
  log_likelihood <- 0 #attribute a log likelihood of 0 for now
  
  # Helper function to compute total score probability using the sum algorithm
  sum_algorithm_prob <- function(betas, score) {
    results <- matrix(1, nrow = J + 1, ncol = J + 1) # make a results matrix filled with ones with J+1 columns and rows, where J is the number of items. 
    bi <- exp(-betas) #set b_i as exp(-Beta_i) for higher understandability 
    
    # Fill the dynamic programming matrix
    for (i in 1:J) {
      for (k in 1:J) {
        results[k + 1, i + 1] <- bi[i] * results[k, i] + results[k + 1, i] # fill in the results matrix with the actual results. 
      }
    }
    
    # Return the probability of getting the observed score
    return(results[score + 1, J + 1])  # +1 to account for the possibility of 0 score
  }
  
  bi <- exp(-beta)  # Compute exp(-beta) for all items; THIS IS THE SECOND TIME WE SET THIS, and idk if it is neccesary? I don't believe the betas changed 
  
  # Compute the likelihood for each person using the provided formula
  for (i in 1:N) {
    observed_score <- S_i[i]  # Total score for person i
    prob_score <- sum_algorithm_prob(beta, observed_score)  # Probability of the score
    log_likelihood <- log_likelihood + sum(response_matrix[i, ] * log(bi)) - log(prob_score) # computing the log likelihood, replacing the previous 0 
  }
  
  return(log_likelihood)  # Return the log-likelihood
}

# ------------------------------------------------------------------------------

# Define the gradient (first derivative) of the log-likelihood using the sum algorithm
grad_rasch <- function(beta, response_matrix) {
  N <- nrow(response_matrix) # Number of persons
  J <- ncol(response_matrix) # Number of items

  
  S_i <- rowSums(response_matrix)  # Total correct responses per person
  gradient <- numeric(J) # create vector of 0s of length J 
  
  bi <- exp(-beta)  # Compute exp(-beta) for all items
  
  # Helper function to compute total score probability using the sum algorithm (SAME AS BEFORE)
  sum_algorithm_prob <- function(betas, score) {
    results <- matrix(1, nrow = length(betas) + 1, ncol = length(betas) + 1) # make a results matrix filled with ones with J+1 columns and rows, where J is the number of items. 
    bi <- exp(-betas) #set b_i as exp(-Beta_i) for higher understandability 
    
    for (i in 1:length(betas)) {
      for (k in 1:length(betas)) {
        results[k + 1, i + 1] <- bi[i] * results[k, i] + results[k + 1, i] # fill in the results matrix with the actual results. 
      }
    }
    return(results[score + 1, length(betas) + 1])  # +1 to account for the possibility of 0 score
  }
  
  # Compute the gradient for each beta_j
  for (j in 1:J) {
    grad_sum <- 0 
    for (i in 1:N) {
      observed_score <- S_i[i]  # Total score for person i
      prob_score <- sum_algorithm_prob(beta, observed_score)  # Probability of the total score
      beta_without_j <- beta[-j] # Remove current beta for the derivative following beta
      prev_prob <- sum_algorithm_prob(beta_without_j, observed_score-1)
      grad_sum <- grad_sum + (response_matrix[i, j]/bi[j]) - prev_prob/prob_score  # Adjust with sum algorithm
    }
    gradient[j] <- grad_sum
  }
  
  return(gradient)  # Return the gradient
}

# ------------------------------------------------------------------------------

# Define the Hessian (second derivative) for the Newton-Raphson update, using sum algorithm
hessian_rasch <- function(beta, response_matrix) {
  N <- nrow(response_matrix) # Number of persons
  J <- ncol(response_matrix) # Number of items

  S_i <- rowSums(response_matrix) # Total correct responses per person
  hessian <- matrix(0, J, J)  # Initialize Hessian matrix
  
  bi <- exp(-beta) # Compute exp(-beta) for all items
  
  # Helper function to compute total score probability using the sum algorithm (SAME AS BEFORE)
  sum_algorithm_prob <- function(betas, score) {
    results <- matrix(1, nrow = length(betas) + 1, ncol = length(betas) + 1) # make a results matrix filled with ones with J+1 columns and rows, where J is the number of items. 
    bi <- exp(-betas) #set b_i as exp(-Beta_i) for higher understandability 
    
    for (i in 1:length(betas)) {
      for (k in 1:length(betas)) {
        results[k + 1, i + 1] <- bi[i] * results[k, i] + results[k + 1, i] # fill in the results matrix with the actual results. 
      }
    }
    return(results[score + 1, length(betas) + 1])  # +1 to account for the possibility of 0 score
  }
  
  # Compute the Hessian for each beta_j and beta_k (cross-terms)
  for (j in 1:J) {
    for (k in 1:J) {
      hess_sum <- 0
      for (i in 1:N) {
        observed_score <- S_i[i]  # Total score for person i
        prob_score <- sum_algorithm_prob(beta, observed_score)  # Probability of the total score
        if (j == k) {
          # Diagonal Hessian (second derivative with respect to the same beta_j)
          beta_without_j <- beta[-j] # Remove current beta j = k for the derivative following beta j = k
          prev_prob <- sum_algorithm_prob(beta_without_j, observed_score-1) 
          hess_sum <- hess_sum  - (response_matrix[i, j]/(bi[j])^2) + (prev_prob/prob_score)^2
        } else {
          # Off-diagonal Hessian (cross-term between beta_j and beta_k)
          beta_without_j <- beta[-j] # Remove current beta j for the derivative following beta k
          prev_prob_j <- sum_algorithm_prob(beta_without_j, observed_score-1)
          beta_without_k <- beta[-k] # Remove current beta k for the derivative following beta k
          prev_prob_k <- sum_algorithm_prob(beta_without_k, observed_score-1) 
          beta_without_j_k <- beta[-c(j,k)] # Remove current betas j an k for the derivative following beta k
          prev_prob_j_k <- sum_algorithm_prob(beta_without_j_k, observed_score-2)
          if (observed_score > 2){ # If the total score is 1 then it is not possible to get both item j and k right
            hess_sum <- hess_sum - prev_prob_j_k/prob_score + (prev_prob_j*prev_prob_k)/prev_prob_j_k^2
          } else {
            hess_sum <- hess_sum
          }
        }
        # print(hess_sum)
      }
      hessian[j, k] <- hess_sum
    }
  }
  
  return(hessian)  # Return the Hessian matrix
}

# ------------------------------------------------------------------------------

# Multivariate Newton-Raphson implementation
multivariate_newton_raphson <- function(beta_init, response_matrix, tol = 1e-6, max_iter = 100) {
  beta <- beta_init
  for (iter in 1:max_iter) {
    print(iter) # To track progress
    gradient <- grad_rasch(beta, response_matrix)
    hessian <- hessian_rasch(beta, response_matrix)
    
    # Update rule: beta_new = beta_old - H_inv * grad
    beta_new <- beta - ginv(hessian) %*% gradient
    
    # Check for convergence
    if (max(abs(beta_new - beta)) < tol) {
      cat("Converged after", iter, "iterations\n")
      return(beta_new)
    }
    
    # Update beta
    beta <- beta_new
  }
  
  cat("Did not converge within the maximum number of iterations\n")
  return(beta)
}

# Initial guess for beta
beta_init <- rep(0, ncol(response_matrix))

# Run the multivariate Newton-Raphson algorithm
beta_estimated <- multivariate_newton_raphson(beta_init, response_matrix)

# Print the final estimated beta values
print(beta_estimated)

# Compare the estimated beta with the true beta
comparison <- data.frame(
  Item = 1:length(beta_true),
  True_Beta = beta_true,
  Estimated_Beta = beta_estimated
)

# Display the comparison
print(comparison)
```

The estimation
```{r}
# Run the estimation function
beta_estimated <- est(response_matrix)

# Compare estimated beta with true beta
comparison <- data.frame(
  Item = 1:length(beta_true),
  True_Beta = beta_true,
  Estimated_Beta = beta_estimated
)

# Display the comparison
print(comparison)
```

Compare with in-built function

```{r}
library(VGAMextra)

# Wrapper function that ensures response_matrix is passed
logLik_rasch_wrapper <- function(beta) {
  return(logLik_rasch(beta, response_matrix))
}

grad_rasch_wrapper <- function(beta) {
  return(grad_rasch(beta, response_matrix))
}

# Now call newtonRaphson.basic using these wrapper functions
a <- rep(-10, ncol(response_matrix))  # Initial guess for beta
b <- rep(10, ncol(response_matrix))  # Initial guess for beta

result <- newtonRaphson.basic(
  f = logLik_rasch_wrapper,   # Objective function
  fprime = grad_rasch_wrapper,  # Gradient function
  a = a,  # Starting point (initial beta)
  b = b,
  tol = 1e-6,  # Tolerance for convergence
  n.Seq = 20,  # Optional: step sequence
  nmax = 100  # Maximum number of iterations
)
```