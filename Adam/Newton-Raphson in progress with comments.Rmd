---
title: "Rasch-model"
author: "Merlin"
date: "2024-10-09"
output:
  html_document: default
  pdf_document: default
---

# Load Packages

```{r}
# Required package for matrix operations
library(MASS)  # For the ginv() function (generalized inverse)
```

# The data simulation

```{r}
generate_irt_data <- function(num_items = 3, num_persons = 1000, seed = 124) {
  
  set.seed(seed) # Set seed for reproducibility
  
  # Step 1: Define Item Parameters
  beta_true <- runif(num_items, -2, 2) # uniformly distributed item difficulties around 0 logits
  beta_true <- scale(beta_true, scale = FALSE) # Center the betas because they are relative
  
  # Step 2: Define Person Parameters with fixed mean and SD
  person_ability <- rnorm(num_persons, mean = -1, sd = 1) # normally distributed person abilities
  person_ability <- scale(person_ability, scale = FALSE) # Center the thetas because they are relative
  
  # Step 4: Simulate Responses
  # Create an empty response matrix to store responses for each person-item pair
  response_matrix <- matrix(0, nrow = num_persons, ncol = num_items)
  
  for (i in 1:num_persons) {
    for (j in 1:num_items) {
      # Step 4A: Generate a random number U from uniform [0,1]
      U <- runif(1)
      
      # Step 4B: Compute probability of failure
      prob_failure <- 1 / (1 + exp(person_ability[i] - beta_true[j]))
      
      # Step 4C: Check if U > Probability of failure
      if (U > prob_failure) {
        response_matrix[i, j] <- 1 # Correct response
      } else {
        response_matrix[i, j] <- 0 # Incorrect response
      }
    }
  }
  
  # Return both the response matrix and the true beta values
  return(list(response_matrix = response_matrix, beta_true = beta_true))
}
```

## Generate Example Data
```{r}
# Generate example data with 5 items and 1000 persons
result <- generate_irt_data(num_items = 5, num_persons = 1000)

response_matrix <- result$response_matrix
beta_true <- result$beta_true
```

# The Estimation

### Sum Algorithm for the Rasch Model

* **Initialize Matrix**
* **Compute Item Difficulty Transformation**
* **Fill Matrix Using Recurrence Relation**
* **Return Probability of Observed Score**

```{r}
# Define the sum algorithm for the Rasch model
sum_algorithm_prob <- function(beta, score) {
  # Initialize a matrix with zeros, size (J+1) x (J+1), where J is the number of items
  results <- matrix(0, nrow = length(beta) + 1, ncol = length(beta) + 1) 
  
  # Set the first row to 1 (base case for score accumulation)
  results[1, ] <- 1
  
  # Calculate transformed item difficulties as exp(-beta_i)
  bi <- exp(-beta)
  
  # Fill in the results matrix using the recurrence relation
  for (i in 1:length(beta)) {
    for (k in 1:length(beta)) {
      results[k + 1, i + 1] <- bi[i] * results[k, i] + results[k + 1, i]
    }
  }
  
  # Return the probability for the given score
  return(results[score + 1, length(beta) + 1])  # +1 to handle score of 0
}
```

### Gradient for the Rasch Model
* **Define Inputs**
* **Initialise Variables**
* **Calculate Total Scores**
* **Initialise Gradient Vector**
* **Calculate Gradient for Each Item**
* **Update Gradient Vector**
* **Store and Return Gradient Vector**

```{r}
# Define the gradient (first derivative) of the log-likelihood using the sum algorithm
grad_rasch <- function(beta, response_matrix) {
  N <- nrow(response_matrix)  # Number of persons
  J <- ncol(response_matrix)  # Number of items
  
  S_i <- rowSums(response_matrix)  # Total correct responses per person
  gradient <- numeric(J)  # Initialize gradient vector of length J
  
  bi <- exp(-beta)  # Calculate exp(-beta) for each item
  
  # Calculate the gradient for each beta_j
  for (j in 1:J) {
    grad_sum <- 0  # Initialize sum for current beta_j
    
    for (i in 1:N) {
      observed_score <- S_i[i]  # Person's total score
      prob_score <- sum_algorithm_prob(beta, observed_score)  # Probability of observed score
      beta_without_j <- beta[-j]  # Exclude current beta for partial derivative
      prev_prob <- sum_algorithm_prob(beta_without_j, observed_score - 1)  # Previous probability
      
      # Update grad_sum only if observed_score > 0
      if (observed_score > 0) {
        grad_sum <- grad_sum + (-response_matrix[i, j]) + (bi[j] * prev_prob) / prob_score
      }
    }
    
    gradient[j] <- grad_sum  # Store gradient for beta_j
  }
  
  return(gradient)  # Return gradient vector
}
```
### Hessian for the Rasch Model
* **Define Inputs**
* **Initialise Variables**
* **Initialise Hessian Sum for Item Pairs**
* **Calculate Diagnonal Elements**
* **Calculate Off-Diagonal Elements**
* **Update Hessian Matrix**
* **Return Hessian Matrix**
```{r}
# Define the Hessian (second derivative) for the Newton-Raphson update using the sum algorithm
hessian_rasch <- function(beta, response_matrix) {
  N <- nrow(response_matrix)  # Number of persons
  J <- ncol(response_matrix)  # Number of items

  S_i <- rowSums(response_matrix)  # Total correct responses per person
  hessian <- matrix(0, J, J)  # Initialize Hessian matrix with zeros
  
  bi <- exp(-beta)  # Calculate exp(-beta) for each item
  
  # Compute the Hessian for each pair of beta_j and beta_k (cross-terms)
  for (j in 1:J) {
    for (k in 1:J) {
      hess_sum <- 0  # Initialize the sum for the Hessian entry
      
      for (i in 1:N) {
        observed_score <- S_i[i]  # Total score for person i
        prob_score <- sum_algorithm_prob(beta, observed_score)  # Probability of observed score

        # Diagonal elements: when j equals k
        if (j == k) {
          if (observed_score > 0) {  # Only calculate if the score allows for this derivative
            beta_without_j <- beta[-j]  # Exclude beta_j for the partial derivative
            prev_prob <- sum_algorithm_prob(beta_without_j, observed_score - 1)  # Probability for score - 1
            hess_sum <- hess_sum - (bi[j] * prev_prob) / prob_score + ((bi[j] * prev_prob) / prob_score)^2
          }
          
        # Off-diagonal elements: when j is not equal to k
        } else {
          if (observed_score > 1) {  # Only calculate if the score allows for this derivative
            beta_without_j_k <- beta[-c(j, k)]  # Exclude both beta_j and beta_k
            prev_prob_j_k <- sum_algorithm_prob(beta_without_j_k, observed_score - 2)  # Probability for score - 2
            prev_prob_j <- sum_algorithm_prob(beta[-j], observed_score - 1)  # Probability for score - 1 excluding beta_j
            prev_prob_k <- sum_algorithm_prob(beta[-k], observed_score - 1)  # Probability for score - 1 excluding beta_k
            hess_sum <- hess_sum - (bi[j] * bi[k] * prev_prob_j_k) / prob_score + (bi[j] * bi[k] * prev_prob_j * prev_prob_k) / prob_score^2
          }
        }
      }
      hessian[j, k] <- hess_sum  # Store the calculated value in the Hessian matrix
    }
  }
  
  return(hessian)  # Return the Hessian matrix
}
```
### Multivariate Newton-Raphson Algorithm for the Rasch Model

* **Define Inputs**
* **Initialise Variables**
* **Calculate Gradient and Hessian**
* **Update Beta**
* **Check for Convergence**
* **Return Estimated Beta Values**
```{r}
# Multivariate Newton-Raphson implementation
multivariate_newton_raphson <- function(beta_init, response_matrix, tol = 1e-6, max_iter = 1000, save_iterations = FALSE) {
  beta <- beta_init
  J <- length(beta_init)  # Number of items
  beta_history <- matrix(NA, nrow = max_iter, ncol = J)  # Matrix to store beta values per iteration
  
  for (iter in 1:max_iter) {
    print(iter)  # Track iteration progress
    
    # Calculate gradient and Hessian
    gradient <- grad_rasch(beta, response_matrix)
    hessian <- hessian_rasch(beta, response_matrix)
    
    # Update beta: beta_new = beta - H_inv * gradient
    beta_new <- beta - ginv(hessian) %*% gradient
    print(max(abs(beta_new - beta)))  # Display max change in beta for convergence check
    
    # Save current beta values if required
    if (save_iterations) {
      beta_history[iter, ] <- beta
    }
    
    # Check for convergence
    if (max(abs(beta_new - beta)) < tol) {
      cat("Converged after", iter, "iterations\n")
      # Trim beta_history to actual iterations if saving
      if (save_iterations) {
        beta_history <- beta_history[1:iter, ]
      }
      return(list(beta = beta_new, beta_history = beta_history))
    }
    
    # Update beta for next iteration
    beta <- beta_new
  }
  
  cat("Did not converge within the maximum number of iterations\n")
  return(list(beta = beta, beta_history = beta_history))
}
```

## Application of the Newton-Raphson Algorithm

```{r}
# Initial guess for beta
beta_init <- rep(0, ncol(response_matrix))

# Run the multivariate Newton-Raphson algorithm
beta_output <- multivariate_newton_raphson(beta_init, response_matrix, save_iterations = TRUE)

# Print the final estimated beta values
beta_estimated <- beta_output$beta
print(beta_estimated)

# Compare the estimated beta with the true beta
comparison <- data.frame(
  Item = 1:length(beta_true),
  True_Beta = beta_true,
  Estimated_Beta = beta_estimated
)

# Display the comparison
print(comparison)
```

The comparison with the rasch model

```{r}
library(ltm)
rasch_model <- rasch(response_matrix)
beta_estimates_ltm <- coef(rasch_model)
summary(rasch_model)
beta_estimates_ltm
```

Graph that shows convergence:

```{r}
library(ggplot2) # For plotting
library(reshape2) # For melt

# Since we already saved the beta history, we can plot it:
beta_history <- beta_output$beta_history

# Convert beta_history to a data frame for plotting
beta_history_df <- as.data.frame(beta_history)
beta_history_df$Iteration <- 1:nrow(beta_history_df)
beta_history_long <- reshape2::melt(beta_history_df, id.vars = "Iteration", variable.name = "Item", value.name = "Beta")

# Rename the Item variable for clearer legend labels
beta_history_long$Item <- factor(beta_history_long$Item, labels = paste0("Item ", 1:ncol(beta_history)))

# Plot convergence of each beta value with customized legend labels
ggplot(beta_history_long, aes(x = Iteration, y = Beta, color = Item)) +
  geom_line() +
  labs(title = "Convergence of beta values across iterations", x = "Iteration", y = "Beta Estimate", color = "Item") +
  theme_minimal()
```
