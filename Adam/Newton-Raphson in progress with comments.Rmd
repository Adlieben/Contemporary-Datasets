---
title: "Rasch-model"
author: "Adam Maghout, Jasmijn Bazen, Jonathan Koop and Merlin Urbanski"
date: "2024-10-09"
output:
  html_document: default
  pdf_document: default
---

# Introduction and outline

The aim of the project is to estimate the item parameters of a Rasch model without making assumptions concerning the person parameters. In order to achieve this, we first generate data for which the true item parameters are known, then construct a function to re-estimate these parameters using a Newton-Raphson algorithm. Checks are performed along the way to ensure that the data generation is performed correctly and that the constructed algorithm produces stable estimates that correspond to the desired item parameters. These are compared to the original values and benchmarked against the values produced by the `rasch()` function in the `ltm` package to demonstrate that the algorithm is functioning properly. Finally, data is generated for varying number of items, number of respondents and methods of generation to investigate the function's overall performance. This is measured using mean average error (MAE).

# Load Packages

```{r, warning=FALSE, message=FALSE}
library(ltm)   # For rasch() function
library(tidyverse) # For plotting
library(reshape2) # For melt 
library(knitr) # For constructing data
library(kableExtra) # For constructing data
library(reshape2) # For matrix operations
```

# Data 

## Function for Generating Data

For the data we generate, we assume there are true values for person and item location parameters. These will be useful to compare to the output of our item parameter retrieval algorithm later on. Both sets of parameters are scaled beforehand to reflect the relativety of location parameters during estimation. For the time being, we assume no missing data, signifying that only one booklet is administered and that every person managed to complete every item.

```{r}
generate_irt_data <- function(num_items = 3, num_persons = 1000,
                              diff_dist = c("normal", "t", "uniform"), sd = NA, df = NA, min = NA, max = NA,
                              seed = 124) {
  
  set.seed(seed) # Set seed for reproducibility
  
  # Step 1: Define Item Parameters
  
  if (diff_dist == "normal" & is.na(sd)) {
    stop("Please specify a standard deviation for the normal distribution.")
  } else if (diff_dist == "t" & is.na(df)) {
    stop("Please specify degrees of freedom for the t-distribution.")
  } else if (diff_dist == "uniform" & (is.na(min) | is.na(max))) {
    stop("Please specify a minimum and maximum value for the uniform distribution.")
  } else if (diff_dist == "normal") {
    beta_true <- rnorm(num_items, 0, sd) # normally distributed item difficulties around 0 logits
  } else if (diff_dist == "t") {
    beta_true <- rt(num_items, df) # t-distributed item difficulties
  } else if (diff_dist == "uniform") {
    beta_true <- runif(num_items, min, max) # uniformly distributed item difficulties
  }  else {
    stop("Invalid distribution specified. Please choose 'normal' (with specified sd) or 't' (with specified df).")
  }
  
  beta_true <- scale(beta_true, scale = FALSE) # Center the betas because they are relative
  
  # Step 2: Define Person Parameters with fixed mean and SD
  person_ability <- rnorm(num_persons, mean = 0, sd = 1) # normally distributed person abilities
  person_ability <- scale(person_ability, scale = FALSE) # Center the thetas because they are relative
  
  # Step 4: Simulate Responses
  # Create an empty response matrix to store responses for each person-item pair
  response_matrix <- matrix(NA, nrow = num_persons, ncol = num_items)
  
  for (i in 1:num_persons) {
    for (j in 1:num_items) {
      # Step 4A: Generate a random number U from uniform [0,1]
      U <- runif(1)
      
      # Step 4B: Compute probability of failure
      prob_failure <- 1 / (1 + exp(person_ability[i] - beta_true[j]))
      
      # Step 4C: Check if U > Probability of failure
      if (U > prob_failure) {
        response_matrix[i, j] <- 1 # Correct response
      } else {
        response_matrix[i, j] <- 0 # Incorrect response
      }
    }
  }
  
  # Return both the response matrix and the true beta values
  return(list(response_matrix = response_matrix, beta_true = beta_true, person_ability = person_ability))
}
```

## Data Generation

```{r}
# Generate example data with 5 items and 1000 persons (standard normal distribution for item difficulties)
result <- generate_irt_data(num_items = 5, num_persons = 1000, diff_dist = "normal", sd = 1)

response_matrix <- result$response_matrix
beta_true <- result$beta_true
person_ability <- result$person_ability
```

## Checking the Data

We can verify that the data generation process has worked by plotting characteristic curves for each item.

```{r, warning=F, message=F}
# Create a data frame from response matrix for plotting
response_df <- as.data.frame(response_matrix)
colnames(response_df) <- paste0("Item_", 1:ncol(response_df))
response_df$Ability <- person_ability

# Gather data for ggplot (long format)
response_long <- response_df %>%
  tidyr::pivot_longer(cols = starts_with("Item_"),
                      names_to = "Item",
                      values_to = "Response")

# Plot the raw responses (0 or 1) with a fitted logistic curve for each item
p <- ggplot(response_long, aes(x = Ability, y = Response, color = Item)) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) +  # Fitted logistic curve
  theme_minimal() +
  labs(title = paste0("Characteristic curves for items 1-", ncol(response_df)-1),
       x = "Person Ability",
       y = "Score (0 = Incorrect, 1 = Correct)")

# Overlay vertical lines for each item's beta
for (i in 1:length(beta_true)) {
  p <- p + geom_vline(xintercept = beta_true[i], linetype = "dashed", color = scales::hue_pal()(length(beta_true))[i])
}

# Display the plot
print(p)
```

Here, the item difficulty parameters are represented on the intersection of the x-axis with the vertical lines. It is clear that the probability of answering an item correctly is dictated by the item difficulty parameters so the data generation process has worked. This can be confirmed by the fact that the logistic curves all predict a probability of 0.5 at the respective item difficulty parameter.

# The Estimation

### Rationale

The probability of observing a pattern $x$ of $n$ responses for a person $p$ under a Rasch model is:

\begin{equation}
P(X = x | \theta_p, \beta_i) = \prod_{i=1}^n \frac{\exp(x_i (\theta_p - \beta_i))}{1 + \exp(\theta_p - \beta_i)}
\end{equation}

Where $\theta_p$ is the ability of the person and $\beta_i$ is the difficulty of the item $i$ with $i$ ranging from 1 to n. Here, we are interested in estimating the item location parameters $\beta_i$. Given an observed score pattern $x$, this entails finding values for the item parameters that are most likely to have produced the pattern. In other words, we want to maximise the likelihood of the $\beta_i$ given $x$. To this goal, we transform equation (1) to represent the probability of a total score $x^+$ being equal to a value $r$. This is simply the sum of all possible combinations of $x_i$ that lead to $x^+=r$, that is:

\begin{equation}
\begin{split}
P(x^+ = r | \theta, \beta) & = \sum_{x\rightarrow r} \prod_{i=1}^n \frac{\exp(x_i (\theta_p - \beta_i))}{1 + \exp(\theta_p - \beta_i)} \\~\\
& = \sum_{x\rightarrow r} \frac{\prod_{i=1}^n \exp(\theta_p x_i) \prod_{i=1}^n \exp(-\beta_ix_i)}{\prod_{i=1}^n 1 + \exp(\theta_p - \beta_i)} \\~\\
& = \frac{\exp(\theta_pr)}{\prod_{i=1}^n 1 + \exp(\theta_p - \beta_i)}\sum_{x\rightarrow r}\prod_{i=1}^n \exp(-\beta_ix_i))
\end{split}
\end{equation}

The last transformation reveals the utility of first finding an expression of the probability of a total score before computing the desired likelihood: we now have two parts, one that depends on the item difficulties, the other on person abilities. Since the person abilities are assumed in this problem to be unknown because we are simply observing a random sample, finding an expression of the likelihood of the item parameters that is independent of them is important. On the other hand, $r$ is known for our score pattern since it is just the sum of the $x_i$ (indeed this property is even used for one of the steps) so it will be more useful to work with. To evaluate $\sum_{x\rightarrow r}\prod_{i=1}^n \exp(-\beta_ix_i))$, we observe that it is equal to the likelihood of each final score $r$, that we can notate $y_r$. Furthermore, for each item $i$, given that $x_i$ can be either 0 or 1, we find that $y_r = \exp(-\beta_i * 0)y_r^{\setminus i} + \exp(-\beta_i * 1)y_{r-1}^{\setminus i} = y_r^{\setminus i} + \exp(-\beta_i)y_{r-1}^{\setminus i}$, where $\setminus i$ indicates the observation $i$ is left out of the calculation. This is trivial as the sum $r$ can be reached both if item $i$ is answered correctly or not. Applying this rule recursively allows us to quickly obtain the likelihood of obtaining each sum $r$. This process will be refered to as the sum algorithm. 

Once this has been established, it can be noted via conditional probability that:

\begin{equation}
\begin{split}
P(x^+ = r | \theta_p, \beta_i) & = \frac{\exp(\theta_p r)}{\prod \left(\exp(\theta_p - \beta_i) + 1\right)} * y_{x^+} \\~\\
P(x^+ = r, X_i = 1 | \theta_p, \beta_i) & = \frac{\exp(\theta_p r)}{\prod \exp(\theta_p - \beta_i) + 1}\exp(-\beta_i)y_{r-1}^{\setminus i} \\~\\
P(x^+ = r, X_i = 0 | \theta_p, \beta_i) & = \frac{\exp(\theta_p r)}{\prod \exp(\theta_p - \beta_i) + 1}y_r^{\setminus i} \\~\\
\Leftrightarrow P(x_i = 1 | \theta_p, \beta_i, x^+) & = \frac{P(x^+ = r, X_i = 1 | \theta_p, \beta_i)}{P(x^+ = r | \theta_p, \beta_i)} = \frac{\exp(-\beta_i)y_{r-1}^{\setminus i}}{y_{x^+}} \\~\\
P(x_i = 0 | \theta_p, \beta_i, x^+) & = \frac{P(x^+ = r, X_i = 0 | \theta_p, \beta_i)}{P(x^+ = r | \theta_p, \beta_i)} = \frac{y_r^{\setminus i}}{y_{x^+}} = 1
\end{split}
\end{equation}

Such that a general form of $P(X = x | \theta_p, \beta_i, x^+)$ is

\begin{equation}
P(X = x | \theta_p, \beta_i, x^+) = \frac{\prod_{i=1}^n \exp(-\beta_i x_{i})}{y_{r}} = \frac{\prod_{i=1}^n b_i^{xî}}{y_{r}}
\end{equation}

where $b_i = \exp(-\beta_i)$. By taking this formula across all persons p, this yields the desired likelihood as a function of only the item parameters and observed scores:

\begin{equation}
L = \prod_{p=1}^s\frac{\prod_{i=1}^n b_i^{x^{pi}}}{y_{x^+p}}
\end{equation}

for $s$ persons. Equivalently, this can be expressed as a log-likelihood that we seek to maximise:

\begin{equation}
l = \log L = \sum_{i=1}^n \sum_{p=1}^s -\beta_i x_{pi} - \sum_{p} \log({y_{x^+ p}})
\end{equation}

By noticing that $\frac{\partial y_{x^+ p}}{\partial \beta_i} = \frac{\partial}{\partial \beta_i}(y_r^{\setminus i} + \exp(-\beta_i)y_{r-1}^{\setminus i}) = -\exp(-\beta_i)y_{r-1}^{\setminus i}$, we can construct the following gradient for the log-likelihood defined above:

\begin{equation}
\frac{\partial l}{\partial \beta_i} = - \sum_{p=1}^sx_{pi} + \frac{\exp(-\beta_i)y_{r-1}^{\setminus i}}{y_{x^+ p}}
\end{equation}

The roots of this equation cannot be found as the sum likelihoods involved are also dependent on $\beta_i$. Thus, the only solution to find the vector of item parameters that maximises the log-likelihood for every item is to try different values, evaluate the log-likelihood and choose the values for which the log-likelihood is highest. Given that the item parameters can take on any value, this is an impossible task computationally unless knowledge is already available concerning plausible values. In absence of this, an alternative is to implement the Newton-Raphson algorithm, which cycles through values that the parameters can take on and eventually converges to values that maximise the log-likelihood. At each step of the algorithm, the new proposed value for the likelihood is given by:

\begin{equation}
\beta_{new} = \beta_{old} - l''(\beta_{old})^{-1} l'(\beta_{old})
\end{equation}

where $\beta$ is the vector of item parameters, $l''(\beta)$ is the Hessian matrix, corresponding to the second derivative of the log-likelihood function, and $l'(\beta)$ is the gradient vector defined above. The only difficulty is then to find an analytic formula for the values in the Hessian matrix. This task is simplified again by noting that $\frac{\partial y_{x^+ p}}{\partial \beta_i} = -\exp(-\beta_i)y_{r-1}^{\setminus i}$ such that the diagonal values of the Hessian matrix, for which we differentiate again with respect to $\beta_i$, are given by:

\begin{align}
\frac{\partial^2 l}{\partial \beta_i^2} &= 0 + \frac{\partial l}{\partial \beta_i} \left( \exp(-\beta_i) y_{r-1}^{\setminus i} \right) \frac{1}{y_{x^+ p}} + \exp(-\beta_i) y_{r-1}^{\setminus i} \frac{\partial l}{\partial \beta_i} \left( \frac{1}{y_{x^+ p}} \right) \\[10pt]
&= \frac{-\exp(-\beta_i) y_{r-1}^{\setminus i}}{y_{x^+ p}} + \left( \frac{\exp(-\beta_i) y_{r-1}^{\setminus i}}{y_{x^+ p}} \right)^2
\end{align}

For the off-diagonal elements, a similar process is repeated but the resulting expression is different due to $y_{r-1}^{\setminus i}$ being dependent on $\beta_j$ where $j\neq i$:

\begin{align}
\frac{\partial^2 l}{\partial \beta_i \partial \beta_j} &= 0 + \frac{\partial l}{\partial \beta_j} \left( \exp(-\beta_i) y_{r-1}^{\setminus i} \right) \frac{1}{y_{x^+ p}} + \exp(-\beta_i) y_{r-1}^{\setminus i} \frac{\partial l}{\partial \beta_j} \left( \frac{1}{y_{x^+ p}} \right) \\[10pt]
&= \frac{-\exp(-\beta_i) \exp(-\beta_j) y_{r-2}^{\setminus i,j}}{y_{x^+ p}} + \frac{\exp(-\beta_i) \exp(-\beta_j) y_{r-1}^{\setminus i}y_{r-1}^{\setminus j}}{y_{x^+ p}^2}
\end{align}

Given these analytically derived formulas for the gradient and the Hessian, the item parameters can be updated using the Newton-Raphson algorithm. Convergence of the algorithm can be defined after a set number of iterations or once the change in $\beta$ between one iteration and the next is beneath a specified threshold. Here, both approaches can be combined. Namely, if the change in $\beta$ is negligeable between two iterations, the algorithm will stop but will otherwise continue until it reaches a maximum number of iterations. Both the threshold and maximum number of iterations are arbitrary.

### Sum Algorithm

The sum algorithm is defined recurrently:

* **Initialise Matrix**
* **Fill Matrix Using Recurrence Relation**
* **Return Probability of Observed Score**

```{r}
# Define the sum algorithm for the Rasch model
sum_algorithm_prob <- function(beta, score) {
  # Initialize a matrix with zeros, size (J+1) x (J+1), where J is the number of items
  results <- matrix(0, nrow = length(beta) + 1, ncol = length(beta) + 1) 
  
  # Set the first row to 1 (base case for score accumulation)
  results[1, ] <- 1
  
  # Calculate transformed item difficulties as exp(-beta_i)
  bi <- exp(-beta)
  
  # Fill in the results matrix using the recurrence relation
  for (i in 1:length(beta)) {
    for (k in 1:length(beta)) {
      results[k + 1, i + 1] <- bi[i] * results[k, i] + results[k + 1, i]
    }
  }
  
  # Return the probability for the given score
  return(results[score + 1, length(beta) + 1])  # +1 to handle score of 0
}
```

### Gradient for the Rasch Model

The formulation of the gradient vector is given in section 4.0.1:

\begin{equation}
\frac{\partial l}{\partial \beta_i} = - \sum_{p=1}^sx_{pi} + \frac{\exp(-\beta_i)y_{r-1}^{\setminus i}}{y_{x^+ p}}
\end{equation}

The steps to perform this in a computationally efficient manner are:

* **Initialise Variables**
* **Calculate Total Scores**
* **Initialise Gradient Vector**
* **Calculate Gradient for Each Item**
* **Update Gradient Vector**
* **Store and Return Gradient Vector**

```{r}
# Define the gradient (first derivative) of the log-likelihood using the sum algorithm
grad_rasch <- function(beta, response_matrix) {
  N <- nrow(response_matrix)  # Number of persons
  J <- ncol(response_matrix)  # Number of items
  
  S_i <- rowSums(response_matrix)  # Total correct responses per person
  gradient <- numeric(J)  # Initialize gradient vector of length J
  
  bi <- exp(-beta)  # Calculate exp(-beta) for each item
  
  # Calculate the gradient for each beta_j
  for (j in 1:J) {
    grad_sum <- 0  # Initialize sum for current beta_j
    
    for (i in 1:N) {
      observed_score <- S_i[i]  # Person's total score
      prob_score <- sum_algorithm_prob(beta, observed_score)  # Probability of observed score
      beta_without_j <- beta[-j]  # Exclude current beta for partial derivative
      prev_prob <- sum_algorithm_prob(beta_without_j, observed_score - 1)  # Previous probability
      
      # Update grad_sum only if observed_score > 0
      if (observed_score > 0) {
        grad_sum <- grad_sum + (-response_matrix[i, j]) + (bi[j] * prev_prob) / prob_score
      }
    }
    
    gradient[j] <- grad_sum  # Store gradient for beta_j
  }
  
  return(gradient)  # Return gradient vector
}
```
### Hessian for the Rasch Model

The formulation of the elements of the Hessian matrix is given in section 4.0.1. For the diagonal elements, these can be obtained with:

\begin{equation}
\frac{\partial^2 l}{\partial \beta_i^2} = \frac{-\exp(-\beta_i) y_{r-1}^{\setminus i}}{y_{x^+ p}} + \left( \frac{\exp(-\beta_i) y_{r-1}^{\setminus i}}{y_{x^+ p}} \right)^2
\end{equation}

For the off-diagonal elements, the formula is: 

\begin{equation}
\frac{\partial^2 l}{\partial \beta_i \partial \beta_j} = \frac{-\exp(-\beta_i) \exp(-\beta_j) y_{r-2}^{\setminus i,j}}{y_{x^+ p}} + \frac{\exp(-\beta_i) \exp(-\beta_j) y_{r-1}^{\setminus i}y_{r-1}^{\setminus j}}{y_{x^+ p}^2}
\end{equation}

The steps to obtain these elements in a computationally efficient manner are:

* **Initialise Variables**
* **Initialise Hessian Sum for Item Pairs**
* **Calculate Diagnonal Elements**
* **Calculate Off-Diagonal Elements**
* **Update Hessian Matrix**
* **Return Hessian Matrix**

```{r}
# Define the Hessian (second derivative) for the Newton-Raphson update using the sum algorithm
hessian_rasch <- function(beta, response_matrix) {
  N <- nrow(response_matrix)  # Number of persons
  J <- ncol(response_matrix)  # Number of items

  S_i <- rowSums(response_matrix)  # Total correct responses per person
  hessian <- matrix(0, J, J)  # Initialize Hessian matrix with zeros
  
  bi <- exp(-beta)  # Calculate exp(-beta) for each item
  
  # Compute the Hessian for each pair of beta_j and beta_k (cross-terms)
  for (j in 1:J) {
    for (k in 1:J) {
      hess_sum <- 0  # Initialize the sum for the Hessian entry
      
      for (i in 1:N) {
        observed_score <- S_i[i]  # Total score for person i
        prob_score <- sum_algorithm_prob(beta, observed_score)  # Probability of observed score

        # Diagonal elements: when j equals k
        if (j == k) {
          if (observed_score > 0) {  # Only calculate if the score allows for this derivative
            beta_without_j <- beta[-j]  # Exclude beta_j for the partial derivative
            prev_prob <- sum_algorithm_prob(beta_without_j, observed_score - 1)  # Probability for score - 1
            hess_sum <- hess_sum - (bi[j] * prev_prob) / prob_score + ((bi[j] * prev_prob) / prob_score)^2
          }
          
        # Off-diagonal elements: when j is not equal to k
        } else {
          if (observed_score > 1) {  # Only calculate if the score allows for this derivative
            beta_without_j_k <- beta[-c(j, k)]  # Exclude both beta_j and beta_k
            prev_prob_j_k <- sum_algorithm_prob(beta_without_j_k, observed_score - 2)  # Probability for score - 2
            prev_prob_j <- sum_algorithm_prob(beta[-j], observed_score - 1)  # Probability for score - 1 excluding beta_j
            prev_prob_k <- sum_algorithm_prob(beta[-k], observed_score - 1)  # Probability for score - 1 excluding beta_k
            hess_sum <- hess_sum - (bi[j] * bi[k] * prev_prob_j_k) / prob_score + (bi[j] * bi[k] * prev_prob_j * prev_prob_k) / prob_score^2
          }
        }
      }
      hessian[j, k] <- hess_sum  # Store the calculated value in the Hessian matrix
    }
  }
  
  return(hessian)  # Return the Hessian matrix
}
```
### Multivariate Newton-Raphson Algorithm for the Rasch Model

Given the previous functions for the sum algorithm, gradient vector and Hessian matrix, the Newton-Raphson Algorithm is defined as follows:

* **Deal with missing data**
* **Initialise Variables**
* **Calculate Gradient and Hessian**
* **Update Beta**
* **Check for Convergence**
* **Return Estimated Beta Values**

The method for dealing with missing data can be specified in the model below. Three methods are offered:

* **Complete cases**: Only rows in which there is no missing data are used to compute item difficulties. This could be beneficial in situations where the missingness mechanism is MAR or MCAR, for example when a teacher forgot to grade a question.
* **Mean imputation**: Missing values are replaced by the most frequent score for the item, that is the rounded mean for bianry outcomes 0 and 1. This can be used in similar situations as for complete cases but allows the use of the rest of the respondent's grades. The drawback is that this will bias the difficulty parameter of the item where the missingness ocurred. This effect will be especially noticeable when there are many missing values.
* **Replacement by zero**: Missing values are replaced by 0. If the missingess is known to be related to students not being able to or not wanting to answer an item, this is a valid approach.  

```{r}
# Multivariate Newton-Raphson implementation
multivariate_newton_raphson <- function(beta_init, # Initial guess for item parameters
                                        response_matrix, # Response matrix for which parameters are to be estimated
                                        tol = 1e-6, # Threshold after which convergence is reached
                                        max_iter = 1000, # Maximum number of iterations if convergence isn't reached before
                                        NA_method = 'complete', 
                                        # Method for dealing with missing data, by default use complete cases
                                        save_iterations = FALSE # Option to save parameters at each iteration
                                        ) {
  # Check if NA_method is correctly specified
  if (!NA_method %in% c("complete", "mean.imp", "zero")) {
    stop("Invalid NA_method specified. Choose from 'complete', 'mean.imp', or 'zero'.")
  }
  
  # Handle missing data based on specified NA_method
  if (NA_method == "complete") {
    response_matrix <- response_matrix[complete.cases(response_matrix), ]
  } else if (NA_method == "mean.imp") {
    response_matrix <- apply(response_matrix, 2, function(column) {
      ifelse(is.na(column), mean(column, na.rm = TRUE), column)
    })
  } else if (NA_method == "zero") {
    response_matrix[is.na(response_matrix)] <- 0
  }
  
  beta <- beta_init
  J <- length(beta_init)  # Number of items
  beta_history <- matrix(NA, nrow = max_iter, ncol = J)  # Matrix to store beta values per iteration
  
  for (iter in 1:max_iter) {
    # print(iter)  # Track iteration progress (can be turned on optionally)
    
    # Calculate gradient and Hessian
    gradient <- grad_rasch(beta, response_matrix)
    hessian <- hessian_rasch(beta, response_matrix)
    
    # Update beta: beta_new = beta - H_inv * gradient
    beta_new <- beta - solve(hessian) %*% gradient
    # print(max(abs(beta_new - beta)))  # Display max change in beta for convergence check (also optional)
    
    # Save current beta values if required
    if (save_iterations) {
      beta_history[iter, ] <- beta
    }
    
    # Check for convergence
    if (max(abs(beta_new - beta)) < tol) {
      # cat("Converged after", iter, "iterations\n") # Can be used for tracking
      # Trim beta_history to actual iterations if saving
      if (save_iterations) {
        beta_history <- beta_history[1:iter, ]
      }
      return(list(beta = beta_new, beta_history = beta_history))
    }
    
    # Update beta for next iteration
    beta <- beta_new
  }
  
  # cat("Did not converge within the maximum number of iterations\n") # Can be used for tracking
  return(list(beta = beta, beta_history = beta_history))
}
```

## Application of the Newton-Raphson Algorithm

### Choosing the Initial Values

Initial values for the Newton-algorithm must be defined before using it on the data generated earlier. We generally considered two options for this:

1. **Vector of Zeroes**: Given that the item difficulties were centered around 0, a vector of zeroes could be used as the starting point. This is consistent with the data generation process, during which the item difficulties were first centered. However, it may take longer to converge, as the algorithm would need to adjust the values without incorporating any prior knowledge.

2. **Vector With Log-Odds**: Alternatively, a vector of log-odds could be used as the starting point. Specifically, these log-odds would be calculated as $log(\frac{P(X_i=1)}{1-P(X_i=1)})$, where $P(X_i=1)$ is the proportion of correct responses for item $i$. This would provide a more informed starting point for the algorithm, as it would already be close to the true item difficulties.

In order to compare the two methods, we can check how fast the algorithm converges for each starting point, given different parameter sizes. This can be done by running the algorithm for both starting points and comparing the number of iterations required for convergence. Below, we run the algorithm for both starting points and compare the results in convergence plots.

```{r}
table_iterations <- data.frame(sd = numeric(0), 
                               iterations_zero = numeric(0), 
                               iterations_logodds = numeric(0))

sd <- seq(0.25, 2, length.out = 8)

for (i in seq_along(sd)) {
  # Example data with 5 items and 1000 obs (normal distribution for item difficulties)
  result <- generate_irt_data(num_items = 5, num_persons = 1000, diff_dist = "normal", sd = sd[i])
  
  # save sd in table
  table_iterations[i,1] <- sd[i]
  
  response_matrix <- result$response_matrix
  
  # Initial values for beta
  beta_init_zero <- rep(0, ncol(response_matrix))
  beta_init_logodds <- scale(-log(colMeans(response_matrix) / (1 - colMeans(response_matrix))), scale = FALSE)
  
  # Run the multivariate Newton-Raphson algorithm with 0s as initial values
  results_zero <- multivariate_newton_raphson(beta_init_zero, response_matrix, save_iterations = TRUE)
  table_iterations[i,2] <- nrow(results_zero$beta_history)
  
  # Run the multivariate Newton-Raphson algorithm with log-odds initial values
  results_logodds <- multivariate_newton_raphson(beta_init_logodds, response_matrix, save_iterations = TRUE)
  table_iterations[i,3] <- nrow(results_logodds$beta_history) # record the number of iterations
}

table_iterations <- round(table_iterations, 2)
# turn table into nice kable
kable(table_iterations, 
      col.names = c("Standard Deviation", "Iterations (Zeros)", "Iterations (Log-Odds)"),
      caption = "Number of iterations required for convergence with different starting values for item difficulties")
```

As can be retrieved from the table above, the number of iterations required for convergence is generally lower when using log-odds as the initial values. This is consistent with the fact that the algorithm converges faster when starting from a more informed point. There is also a clear trend, revealing that the advantage of the log-odds approach increases as the standard deviation of the item difficulties increases. This is logical, as the log-odds approach provides a better starting point when the item difficulties are further from 0.


### Running the Newton-Raphson Algorithm

Below, we run the Newton-Raphson algorithm using the log-odds as the initial values and compare the estimated item difficulties to the true values.

```{r}
# Generate example data with 5 items and 1000 persons (normal distribution for item difficulties)
result <- generate_irt_data(num_items = 5, num_persons = 1000, diff_dist = "normal", sd = 0.5)

response_matrix <- result$response_matrix # Response matrix

# Initial values for beta
beta_init <- scale(-log(colMeans(response_matrix) / (1 - colMeans(response_matrix))), scale = FALSE)

# Run the multivariate Newton-Raphson algorithm
beta_output <- multivariate_newton_raphson(beta_init, response_matrix, save_iterations = TRUE)

# Print the final estimated beta values
beta_estimated <- beta_output$beta
print(beta_estimated)

# Compare the estimated beta with the true beta
comparison <- data.frame(
  Item = 1:length(beta_true),
  True_Beta = result$beta_true,
  Estimated_Beta = beta_estimated
)

# Display the comparison
print(comparison)
```


The comparison given above indicates that the algorithm is succesful as similar values to those used for the data generation are found. We can also use the `rasch` function from the `ltm` package to get item difficulty parameters, which instead of conditional maximum likelihood estimation (CMLE) uses marginal maximum likelihood estimation (MMLE):


```{r}
rasch_model <- rasch(response_matrix)
beta_estimates_ltm <- coef(rasch_model)
beta_estimates_ltm
```

Again, there is little difference between our estimated parameters via the rasch function and the true parameters. This also confirms that retrieving the original parameters perfectly is not possible given random variation in the response matrix. Finally, we can verify that the Newton-Raphson function is converging properly and that the values observed are not due to chance or convergence in a local maximum. This can be done easily as the function already stores the values of $\beta$ at each iteration:

```{r}
# Since we already saved the beta history, we can plot it:
beta_history <- beta_output$beta_history

# Convert beta_history to a data frame for plotting
beta_history_df <- as.data.frame(beta_history)
beta_history_df$Iteration <- 1:nrow(beta_history_df)
beta_history_long <- reshape2::melt(beta_history_df, id.vars = "Iteration", variable.name = "Item", value.name = "Beta")

# Rename the Item variable for clearer legend labels
beta_history_long$Item <- factor(beta_history_long$Item, labels = paste0("Item ", 1:ncol(beta_history)))

# Plot convergence of each beta value with customized legend labels
ggplot(beta_history_long, aes(x = Iteration, y = Beta, color = Item)) +
  geom_line() +
  labs(title = "Convergence of beta values across iterations", x = "Iteration", y = "Beta Estimate", color = "Item") +
  theme_minimal()
```

Convergence is clear as the algorithm quickly reaches item difficulties similar to those outputted as results by the function. This also suggests that a smaller tolerance level could be used in this instance, which can be fixed using the 'tol' parameter in the function.

In addition to that, the plot also reveals the advantage of the log-odds approach, as the initial values chosen (see very left of the traces) are much closer to the estimates than the zero values.

# Evaluation of the Function's Correctness

In order to evaluate, whether the function correctly estimates the item parameters, we can compare the estimated item parameters with the true item parameters in various scenarios, letting the number of items and persons, as well as the item difficulties and their distribution vary. The following table presents the conditions under which the functions were evaluated:

```{r}
table <- data.frame(matrix(ncol = 1, nrow = 3))

rownames(table) <- c("Number of Persons", "Number of Items", "Standard Deviation of Parameters")

table[1,] <- c("100, 1000, 10000")
table[2,] <- c("3, 5, 10")
table[3,] <- c("0.1, 0.5, 1")

kable(table, col.names = "Conditions", caption = "Conditions for the evaluation of the functions correctness") %>%
  kable_styling(full_width = F)
```

Given that there are three different conditions with three different scenarios each, we can evaluate the functions correctness in a total of 27 scenarios. Below these scenarios are presented:

```{r simulation, warning=F, message=F}
person_conditions <- c(100, 1000, 10000)
item_conditions <- c(3, 5, 10)
difficulty_sds <- c(0.1, 0.5, 1)

# Create MAE_array
MAE_array_custom <- array(NA, dim = c(length(person_conditions), length(item_conditions), length(difficulty_sds)))
dimnames(MAE_array_custom) <- list(person_conditions, item_conditions, difficulty_sds) # label the dimensions of the array

MAE_array_ltm <- array(NA, dim = c(length(person_conditions), length(item_conditions), length(difficulty_sds)))
dimnames(MAE_array_ltm) <- list(person_conditions, item_conditions, difficulty_sds) # label the dimensions of the array

for (i in seq_along(person_conditions)) {
  for (j in seq_along(item_conditions)) {
    for (k in seq_along(difficulty_sds)) {
      # print(paste("Scenario:", i, j, k)) # Can be used for tracking
      
      #### Simulate Responses and Estimate Item Parameters for Each Scenario ####
      result <- generate_irt_data(num_items = item_conditions[j], num_persons = person_conditions[i], diff_dist = "normal", sd = difficulty_sds[k])
      response_matrix <- result$response_matrix
      beta_true <- result$beta_true
      
      # Initial values for beta
      beta_init <- scale(-log(colMeans(response_matrix) / (1 - colMeans(response_matrix))), scale = FALSE)
      
      beta_estimated <- multivariate_newton_raphson(beta_init, response_matrix, save_iterations = TRUE)$beta
      
      #### Compute the Mean Absolute Error (MAE) between the True and Estimated Item Parameters ####
      MAE_own <- mean(abs(beta_true - beta_estimated)) # own function
      MAE_ltm <- mean(abs(beta_true - coef(rasch(response_matrix))[,1])) # ltm function
      
      # save MAE in 3 x 3 x 3 array
      MAE_array_custom[i, j, k] <- MAE_own
      MAE_array_ltm[i, j, k] <- MAE_ltm
    }
  }
}
```

The Mean Absolute Error (MAE) between the true and estimated item parameters for each scenario is presented dividing the scenarios into three tables, one for each item difficulty distribution:

```{r sim results}
for (i in seq_along(difficulty_sds)) {
  # Convert matrix to data frame and add row and column names
  df <- as.data.frame(MAE_array_custom[,,i])
  df$Persons <- rownames(df) # Add row names as a column
  df <- melt(df, id.vars = "Persons", variable.name = "Items", value.name = "MAE")
  
  # Ensure Persons and Items are factors for better control over discrete axes
  df$Persons <- factor(df$Persons)
  df$Items <- factor(df$Items)
  
  # Generate the heatmap with annotations
  p <- ggplot(df, aes(x = Persons, y = Items, fill = MAE)) +
    geom_tile(color = "white") + # White borders for clarity
    geom_text(aes(label = round(MAE, 3)), color = "black", size = 3) + # Add MAE values to each tile
    scale_fill_gradient(low = "green", high = "red", limits = c(0, 0.5)) +
    labs(title = paste0("Custom Function: Mean Absolute Error of Estimated Item Parameters \n(Item Difficulty Parameter Standard Deviation: ", difficulty_sds[i], ")"),
         x = "Number of Persons", y = "Number of Items") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels for readability
  
  # Print the plot explicitly
  print(p)
}

for (i in seq_along(difficulty_sds)) {
  # Convert matrix to data frame and add row and column names
  df <- as.data.frame(MAE_array_ltm[,,i])
  df$Persons <- rownames(df) # Add row names as a column
  df <- melt(df, id.vars = "Persons", variable.name = "Items", value.name = "MAE")
  
  # Ensure Persons and Items are factors for better control over discrete axes
  df$Persons <- factor(df$Persons)
  df$Items <- factor(df$Items)
  
  # Generate the heatmap with annotations
  p <- ggplot(df, aes(x = Persons, y = Items, fill = MAE)) +
    geom_tile(color = "white") + # White borders for clarity
    geom_text(aes(label = round(MAE, 3)), color = "black", size = 3) + # Add MAE values to each tile
    scale_fill_gradient(low = "green", high = "red", limits = c(0, 0.5)) +
    labs(title = paste0("ltm Function: Mean Absolute Error of Estimated Item Parameters \n(Item Difficulty Parameter Standard Deviation: ", difficulty_sds[i], ")"),
         x = "Number of Persons", y = "Number of Items") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels for readability
  
  # Print the plot explicitly
  print(p)
}
```

As can be seen in the plots, both the implemented and the `ltm` functions perform similarly in estimating the item parameters. The Mean Absolute Error (MAE) between the true and estimated item parameters is generally low with a sufficient number of observations/person. In the cases of 1000 and 10000 persons, the MAEs are generally around 0.05 implying that the estimates on average only deviate by 0.05 from the true values. This indicates that the implemented functions are quite reliable for estimating item parameters in the Rasch model given exponentially or normally distributed parameters.
In the case of fewer observations (i.e., n=100), the MAEs are generally higher, indicating that the estimates are less reliable. In these cases, the custom function surprisingly performs better than the `ltm` function, as the MAEs are generally lower. This is particularly evident for the scenario with a standard deviation of 1, where the MAEs of `ltm`'s function range up to 0.332, while the custom function's MAEs are below 0.22. This suggests that the custom function is more reliable for estimating item parameters in the Rasch model given normally distributed parameters with fewer observations.

