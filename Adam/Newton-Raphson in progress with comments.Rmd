---
title: "Rasch-model"
author: "Merlin"
date: "2024-10-09"
output: html_document
---
The data simulation
```{r}
# Step 1: Define Item Parameters
set.seed(124) # for reproducibility
num_items <- 3 # number of items
beta_true <- runif(num_items, -2, 2) # uniformly distributed item difficulties around 0 logits
beta_true <- scale(beta_true, scale = F) # Center the betas because they are relative

# Step 2: Define Person Parameters
num_persons <- 1000 # sample size
person_ability <- rnorm(num_persons, -1, 1) # normally distributed person abilities with mean 0 and SD 1
person_ability <- scale(person_ability, scale = F) # Center the thetas because they are relative

# Step 4: Simulate Responses
# Create an empty response matrix to store responses for each person-item pair
response_matrix <- matrix(0, nrow = num_persons, ncol = num_items)

for (i in 1:num_persons) {
  for (j in 1:num_items) {
    # Step 4A: Generate a random number U from uniform [0,1]
    U <- runif(1)
    
    # Step 4B: Compute probability of failure
    prob_failure <- 1 / (1 + exp(person_ability[i] - beta_true[j]))
    
    # Step 4C: Check if U > Probability of failure
    if (U > prob_failure) {
      response_matrix[i, j] <- 1 # Correct response
    } else {
      response_matrix[i, j] <- 0 # Incorrect response
    }
  }
}
```


The estimation

```{r}
# Required package for matrix operations
library(MASS)  # For the ginv() function (generalized inverse)

# Define the sum algorithm for the Rasch model
sum_algorithm_prob <- function(beta, score) {
  results <- matrix(0, nrow = length(beta) + 1, ncol = length(beta) + 1) # make a results matrix filled with ones with J+1 columns and rows, where J is the number of items. 
  results[1, ] <- 1 # set values in the first row to 1
  bi <- exp(-beta) # set b_i as exp(-Beta_i)
  
  for (i in 1:length(beta)) {
    for (k in 1:length(beta)) {
      results[k + 1, i + 1] <- bi[i] * results[k, i] + results[k + 1, i] # fill in the results matrix with the actual results. 
    }
  }
  return(results[score + 1, length(beta) + 1])  # +1 to account for the possibility of 0 score
}

# Define the log-likelihood for all beta values using the sum algorithm
logLik_rasch <- function(beta, response_matrix) {
  N <- nrow(response_matrix)  # Number of persons
  J <- ncol(response_matrix)  # Number of items
  
  S_i <- rowSums(response_matrix)  # Total correct responses per person
  log_likelihood <- 0 # attribute a log likelihood of 0 for now
  
  # Compute the likelihood for each person
  for (i in 1:N) {
    observed_score <- S_i[i]  # Total score for person i
    prob_score <- sum_algorithm_prob(beta, observed_score)  # Probability of the score
    log_likelihood <- log_likelihood + sum(response_matrix[i, ] * (-beta)) - log(prob_score) # using -beta in the log-likelihood
  }
  
  return(log_likelihood)  # Return the log-likelihood
}

# ------------------------------------------------------------------------------

# Define the gradient (first derivative) of the log-likelihood using the sum algorithm
grad_rasch <- function(beta, response_matrix) {
  N <- nrow(response_matrix) # Number of persons
  J <- ncol(response_matrix) # Number of items
  
  S_i <- rowSums(response_matrix)  # Total correct responses per person
  gradient <- numeric(J) # create vector of 0s of length J 
  
  bi <- exp(-beta)  # Compute exp(-beta) for all items
  
  # Compute the gradient for each beta_j
  for (j in 1:J) {
    # print(paste("j i", j))
    grad_sum <- 0 
    for (i in 1:N) {
          # print(paste("i i", i))
      observed_score <- S_i[i]  # Total score for person i
      prob_score <- sum_algorithm_prob(beta, observed_score)  # Probability of the total score
      beta_without_j <- beta[-j] # Remove current beta for the derivative following beta
      prev_prob <- sum_algorithm_prob(beta_without_j, observed_score - 1)
      
      if (observed_score > 0) {  # Failsafe: Only calculate if gradient is defined
        grad_sum <- grad_sum + (-response_matrix[i, j]) + (bi[j] * prev_prob) / prob_score  # Adjust with sum algorithm
      }
      # print(grad_sum)
    }
    gradient[j] <- grad_sum
  }
  
  return(gradient)  # Return the gradient
}

# ------------------------------------------------------------------------------

# Define the Hessian (second derivative) for the Newton-Raphson update, using sum algorithm
hessian_rasch <- function(beta, response_matrix) {
  N <- nrow(response_matrix) # Number of persons
  J <- ncol(response_matrix) # Number of items

  S_i <- rowSums(response_matrix) # Total correct responses per person
  hessian <- matrix(0, J, J)  # Initialize Hessian matrix
  
  bi <- exp(-beta) # Compute exp(-beta) for all items
  
  # Compute the Hessian for each beta_j and beta_k (cross-terms)
  for (j in 1:J) {
    for (k in 1:J) {
      hess_sum <- 0
      for (i in 1:N) {
        observed_score <- S_i[i]  # Total score for person i
        prob_score <- sum_algorithm_prob(beta, observed_score)  # Probability of the total score
        

          if (j == k) {
            if (observed_score > 0) {  # Failsafe: Only calculate if the Hessian is defined
              beta_without_j <- beta[-j] # Remove current beta j = k for the derivative following beta j = k
              prev_prob <- sum_algorithm_prob(beta_without_j, observed_score - 1)
              hess_sum <- hess_sum - (bi[j] * prev_prob) / prob_score + ((bi[j] * prev_prob) / prob_score)^2
            }
          } else {
            if (observed_score > 1) {  # Failsafe: Only calculate if the Hessian is defined
              beta_without_j_k <- beta[-c(j, k)] # Remove current betas j and k for the derivative following beta j and k
              prev_prob_j_k <- sum_algorithm_prob(beta_without_j_k, observed_score - 2)
              prev_prob_j <- sum_algorithm_prob(beta[-j], observed_score - 1)
              prev_prob_k <- sum_algorithm_prob(beta[-k], observed_score - 1)
              hess_sum <- hess_sum - (bi[j] * bi[k] * prev_prob_j_k) / prob_score + (bi[j] * bi[k] * prev_prob_j * prev_prob_k) / prob_score^2
          }
        }
      }
      hessian[j, k] <- hess_sum
    }
  }
  
  return(hessian)  # Return the Hessian matrix
}

# ------------------------------------------------------------------------------

# Multivariate Newton-Raphson implementation
multivariate_newton_raphson <- function(beta_init, response_matrix, tol = 1e-6, max_iter = 1000, save_iterations = FALSE) {
  beta <- beta_init
  J <- length(beta_init)  # Number of items
  beta_history <- matrix(NA, nrow = max_iter, ncol = J)  # Initialize beta history matrix
  
  for (iter in 1:max_iter) {
    print(iter) # To track progress
    gradient <- grad_rasch(beta, response_matrix)
    hessian <- hessian_rasch(beta, response_matrix)
    
    # Update rule: beta_new = beta_old - H_inv * grad
    beta_new <- beta - ginv(hessian) %*% gradient # Use gradient and hessian derived with respect to beta
    print(max(abs(beta_new - beta)))  # Check the max change for convergence
    
    # Save the current beta values if save_iterations is TRUE
    if (save_iterations) {
      beta_history[iter, ] <- beta  # Store the current beta in the matrix
    }
    
    # Check for convergence
    if (max(abs(beta_new - beta)) < tol) {
      cat("Converged after", iter, "iterations\n")
      # Trim the beta_history matrix to the actual number of iterations
      if (save_iterations) {
        beta_history <- beta_history[1:iter, ]
      }
      return(list(beta = beta_new, beta_history = beta_history))
    }
    
    # Update beta
    beta <- beta_new
  }
  
  cat("Did not converge within the maximum number of iterations\n")
  return(list(beta = beta, beta_history = beta_history))
}

# Initial guess for beta
beta_init <- rep(0, ncol(response_matrix))

# Run the multivariate Newton-Raphson algorithm
beta_output <- multivariate_newton_raphson(beta_init, response_matrix, save_iterations = TRUE)

# Print the final estimated beta values
beta_estimated <- beta_output$beta
print(beta_estimated)

# Compare the estimated beta with the true beta
comparison <- data.frame(
  Item = 1:length(beta_true),
  True_Beta = beta_true,
  Estimated_Beta = beta_estimated
)

# Display the comparison
print(comparison)
```

The comparison with the rasch model

```{r}
library(ltm)
rasch_model <- rasch(response_matrix)
beta_estimates_ltm <- coef(rasch_model)
beta_estimates_ltm
```

Graph that shows convergence:

```{r}
library(ggplot2) # For plotting
library(reshape2) # For melt

# Since we already saved the beta history, we can plot it:
beta_history <- beta_output$beta_history

# Convert beta_history to a data frame for plotting
beta_history_df <- as.data.frame(beta_history)
beta_history_df$Iteration <- 1:nrow(beta_history_df)
beta_history_long <- reshape2::melt(beta_history_df, id.vars = "Iteration", variable.name = "Item", value.name = "Beta")

# Rename the Item variable for clearer legend labels
beta_history_long$Item <- factor(beta_history_long$Item, labels = paste0("Item ", 1:ncol(beta_history)))

# Plot convergence of each beta value with customized legend labels
ggplot(beta_history_long, aes(x = Iteration, y = Beta, color = Item)) +
  geom_line() +
  labs(title = "Convergence of beta values across iterations", x = "Iteration", y = "Beta Estimate", color = "Item") +
  theme_minimal()
```



