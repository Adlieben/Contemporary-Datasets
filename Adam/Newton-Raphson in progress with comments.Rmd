---
title: "Rasch-model"
author: "Adam Maghout, Jasmijn Bazen, Jonathan Koop and Merlin Urbanski"
date: "2024-10-09"
output:
  html_document: default
  pdf_document: default
---

# Introduction and outline

The aim of the project is to estimate the item parameters of a Rasch model without making assumptions concerning the person parameters. In order to achieve this, we first generate data for which the true item parameters are known then construct a function to re-estimate these parameters using a Newton-Raphson algorithm. Checks are performed along the way to ensure that the data generation is performed correctly and that the constructed algorithm produces stable estimates that correspond to the desired item parameters. These are compared to the original values and benchmarked against the values produced by the rasch() function in the ltm package to demonstrate that the algorithm is functioning properly. Finally, data is generated for varying number of items, number of respondents and methods of generation to investigate the function's overall performance. This is measured using mean averag error (MAE).

# Load Packages

```{r, warning=FALSE, message=FALSE}
library(MASS)  # For the ginv() function (generalized inverse)
library(ltm)   # For rasch() function
library(tidyverse) # For plotting
library(reshape2) # For melt 
library(knitr) # For constructing data
library(kableExtra) # For constructing data
library(reshape2) # For matrix operations
```

# Data 

## Function for Generating Data

For the data we generate, we assume there are true values for person and item location parameters. These will be useful to compare to the output of our item parameter retrieval algorithm later on. Both sets of parameters are scaled beforehand to reflect the relativety of location parameters during estimation.

```{r}
generate_irt_data <- function(num_items = 3, num_persons = 1000, seed = 124) {
  
  set.seed(seed) # Set seed for reproducibility
  
  # Step 1: Define Item Parameters
  beta_true <- runif(num_items, -2, 2) # uniformly distributed item difficulties around 0 logits
  beta_true <- scale(beta_true, scale = FALSE) # Center the betas because they are relative
  
  # Step 2: Define Person Parameters with fixed mean and SD
  person_ability <- rnorm(num_persons, mean = -1, sd = 1) # normally distributed person abilities
  person_ability <- scale(person_ability, scale = FALSE) # Center the thetas because they are relative
  
  # Step 4: Simulate Responses
  # Create an empty response matrix to store responses for each person-item pair
  response_matrix <- matrix(0, nrow = num_persons, ncol = num_items)
  
  for (i in 1:num_persons) {
    for (j in 1:num_items) {
      # Step 4A: Generate a random number U from uniform [0,1]
      U <- runif(1)
      
      # Step 4B: Compute probability of failure
      prob_failure <- 1 / (1 + exp(person_ability[i] - beta_true[j]))
      
      # Step 4C: Check if U > Probability of failure
      if (U > prob_failure) {
        response_matrix[i, j] <- 1 # Correct response
      } else {
        response_matrix[i, j] <- 0 # Incorrect response
      }
    }
  }
  
  # Return both the response matrix and the true beta values
  return(list(response_matrix = response_matrix, beta_true = beta_true, person_ability = person_ability))
}
```

## Data Generation

```{r}
# Generate example data with 5 items and 1000 persons
result <- generate_irt_data(num_items = 5, num_persons = 1000)

response_matrix <- result$response_matrix
beta_true <- result$beta_true
person_ability <- result$person_ability
```

## Checking the Data

We can verify that the data generation process has worked by plotting characteristic curves for each item.

```{r, warning=F, message=F}
# Create a data frame from response matrix for plotting
response_df <- as.data.frame(response_matrix)
colnames(response_df) <- paste0("Item_", 1:ncol(response_df))
response_df$Ability <- person_ability

# Gather data for ggplot (long format)
response_long <- response_df %>%
  tidyr::pivot_longer(cols = starts_with("Item_"),
                      names_to = "Item",
                      values_to = "Response")

# Plot the raw responses (0 or 1) with a fitted logistic curve for each item
p <- ggplot(response_long, aes(x = Ability, y = Response, color = Item)) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) +  # Fitted logistic curve
  theme_minimal() +
  labs(title = paste0("Characteristic curves for items 1-", ncol(response_df)-1),
       x = "Person Ability",
       y = "Score (0 = Incorrect, 1 = Correct)")

# Overlay vertical lines for each item's beta
for (i in 1:length(beta_true)) {
  p <- p + geom_vline(xintercept = beta_true[i], linetype = "dashed", color = scales::hue_pal()(length(beta_true))[i])
}

# Display the plot
print(p)
```

Here, the item difficulty parameters are represented on the intersection of the x-axis with the vertical lines. It is clear that the probability of answering an item correctly is dictated by the item difficulty parameters so the data generation process has worked. 

# The Estimation

### Rationale

The probability of observing a pattern $x$ of $n$ responses for a person $p$ under a Rasch model is:

\begin{equation}
P(X = x | \theta_p, \beta_i) = \prod_{i=1}^n \frac{\exp(x_i (\theta_p - \beta_i))}{1 + \exp(\theta_p - \beta_i)}
\end{equation}

Where $\theta_p$ is the ability of the person and $\beta_i$ is the difficulty of the item $i$ with $i$ ranging from 1 to n. Here, we are interested in estimating the item locatio parameters $\beta_i$. Given an observed score pattern $x$, this entails finding values for the item parameters that are most likely to have produced the pattern. In other words, we want to maximise the likelihood of the $\beta_i$ given $x$. To this goal, we transform equation (1) to represent the probability of a total score $x^+$ being equal to a value $r$. This is simply the sum of all possible combinations of $x_i$ that lead to $x^+=r$, that is:

\begin{equation}
\begin{split}
P(x^+ = r | \theta, \beta) & = \sum_{x\rightarrow r} \prod_{i=1}^n \frac{\exp(x_i (\theta_p - \beta_i))}{1 + \exp(\theta_p - \beta_i)} \\~\\
& = \sum_{x\rightarrow r} \frac{\prod_{i=1}^n \exp(\theta_p x_i) \prod_{i=1}^n \exp(-\beta_ix_i)}{\prod_{i=1}^n 1 + \exp(\theta_p - \beta_i)} \\~\\
& = \frac{\exp(\theta_pr)}{\prod_{i=1}^n 1 + \exp(\theta_p - \beta_i)}\sum_{x\rightarrow r}\prod_{i=1}^n \exp(-\beta_ix_i))
\end{split}
\end{equation}

The last transformation reveals the utility of first finding an expression of the probability of a total score before computing the desired likelihood: we now have two parts, one that depends on the item difficulties, the other on person abilities. Since the person abilities are assumed in this problem to be unknown because we are simply observing a random sample, finding an expression of the likelihood of the item parameters that is independent of them is important. On the other hand, $r$ is known for our score pattern since it is just the sum of the $x_i$ (indeed this property is even used for one of the steps) so it will be more useful to work with. To evaluate $\sum_{x\rightarrow r}\prod_{i=1}^n \exp(-\beta_ix_i))$, we observe that it is equal to the likelihood of each final score $r$, that we can notate $y_r$. Furthermore, for each item $i$, given that $x_i$ can be either 0 or 1, we find that $y_r = \exp(-\beta_i * 0)y_r^{\setminus i} + \exp(-\beta_i * 1)y_{r-1}^{\setminus i} = y_r^{\setminus i} + \exp(-\beta_i)y_{r-1}^{\setminus i}$, where $\setminus i$ indicates the observation $i$ is left out of the calculation. This is trivial as the sum $r$ can be reached both if item $i$ is answered correctly or not. Applying this rule recursively allows us to quickly obtain the likelihood of obtaining each sum $r$. This process will be refered to as the sum algorithm. 

Once this has been established, it can be noted via conditional probability that:

\begin{equation}
\begin{split}
P(x^+ = r | \theta_p, \beta_i) & = \frac{\exp(\theta_p r)}{\prod \left(\exp(\theta_p - \beta_i) + 1\right)} * y_{x^+} \\~\\
P(x^+ = r, X_i = 1 | \theta_p, \beta_i) & = \frac{\exp(\theta_p r)}{\prod \exp(\theta_p - \beta_i) + 1}\exp(-\beta_i)y_{r-1}^{\setminus i} \\~\\
P(x^+ = r, X_i = 0 | \theta_p, \beta_i) & = \frac{\exp(\theta_p r)}{\prod \exp(\theta_p - \beta_i) + 1}y_r^{\setminus i} \\~\\
\Leftrightarrow P(x_i = 1 | \theta_p, \beta_i, x^+) & = \frac{P(x^+ = r, X_i = 1 | \theta_p, \beta_i)}{P(x^+ = r | \theta_p, \beta_i)} = \frac{\exp(-\beta_i)y_{r-1}^{\setminus i}}{y_{x^+}} \\~\\
P(x_i = 0 | \theta_p, \beta_i, x^+) & = \frac{P(x^+ = r, X_i = 0 | \theta_p, \beta_i)}{P(x^+ = r | \theta_p, \beta_i)} = \frac{y_r^{\setminus i}}{y_{x^+}} = 1
\end{split}
\end{equation}

Such that a general form of $P(X = x | \theta_p, \beta_i, x^+)$ is

\begin{equation}
P(X = x | \theta_p, \beta_i, x^+) = \frac{\prod_{i=1}^n \exp(-\beta_i x_{i})}{y_{r}} = \frac{\prod_{i=1}^n b_i^{xÃ®}}{y_{r}}
\end{equation}

where $b_i = \exp(-\beta_i)$. By taking this formula across all persons p, this yields the desired likelihood as a function of only the item parameters and observed scores:

\begin{equation}
L = \prod_{p=1}^s\frac{\prod_{i=1}^n b_i^{x^{pi}}}{y_{x^+p}}
\end{equation}

for $s$ persons. Equivalently, this can be expressed as a log-likelihood that we seek to maximise:

\begin{equation}
l = \log L = \sum_{i=1}^n \sum_{p=1}^s -\beta_i x_{pi} - \sum_{p} \log({y_{x^+ p}})
\end{equation}

By noticing that $\frac{\partial y_{x^+ p}}{\partial \beta_i} = \frac{\partial}{\partial \beta_i}(y_r^{\setminus i} + \exp(-\beta_i)y_{r-1}^{\setminus i}) = -\exp(-\beta_i)y_{r-1}^{\setminus i}$, we can construct the following gradient for the log-likelihood defined above:

\begin{equation}
\frac{\partial l}{\partial \beta_i} = - \sum_{p=1}^sx_{pi} + \frac{\exp(-\beta_i)y_{r-1}^{\setminus i}}{y_{x^+ p}}
\end{equation}

The roots of this equation cannot be found as the sum likelihoods involved are also dependent on $\beta_i$. Thus, the only solution to find the vector of item parameters that maximises the log-likelihood for every item is to try different values, evaluate the log-likelihood and choose the values for which the log-likelihood is highest. Given that the item parameters can take on any value, this is an impossible task computationally unless knowledge is already available concerning plausible values. In absence of this, an alternative is to implement the Newton-Raphson algorithm, which cycles through values that the parameters can take on and eventually converges to values that maximise the log-likelihood. At each step of the algorithm, the new proposed value for the likelihood is given by:

\begin{equation}
\beta_{new} = \beta_{old} - l''(\beta_{old})^{-1} l'(\beta_{old})
\end{equation}

where $\beta$ is the vector of item parameters, $l''(\beta)$ is the Hessian matrix, corresponding to the second derivative of the log-likelihood function, and $l'(\beta)$ is the gradient vector defined above. The only difficulty is then to find an analytic formula for the values in the Hessian matrix. This task is simplified again by noting that $\frac{\partial y_{x^+ p}}{\partial \beta_i} = -\exp(-\beta_i)y_{r-1}^{\setminus i}$ such that the diagonal values of the Hessian matrix, for which we differentiate again with respect to $\beta_i$, are given by:

\begin{align}
\frac{\partial^2 l}{\partial \beta_i^2} &= 0 + \frac{\partial l}{\partial \beta_i} \left( \exp(-\beta_i) y_{r-1}^{\setminus i} \right) \frac{1}{y_{x^+ p}} + \exp(-\beta_i) y_{r-1}^{\setminus i} \frac{\partial l}{\partial \beta_i} \left( \frac{1}{y_{x^+ p}} \right) \\[10pt]
&= \frac{-\exp(-\beta_i) y_{r-1}^{\setminus i}}{y_{x^+ p}} + \left( \frac{\exp(-\beta_i) y_{r-1}^{\setminus i}}{y_{x^+ p}} \right)^2
\end{align}

For the off-diagonal elements, a similar process is repeated but the resulting expression is different due to $y_{r-1}^{\setminus i}$ being dependent on $\beta_j$ where $j\neq i$:

\begin{align}
\frac{\partial^2 l}{\partial \beta_i \partial \beta_j} &= 0 + \frac{\partial l}{\partial \beta_j} \left( \exp(-\beta_i) y_{r-1}^{\setminus i} \right) \frac{1}{y_{x^+ p}} + \exp(-\beta_i) y_{r-1}^{\setminus i} \frac{\partial l}{\partial \beta_j} \left( \frac{1}{y_{x^+ p}} \right) \\[10pt]
&= \frac{-\exp(-\beta_i) \exp(-\beta_j) y_{r-2}^{\setminus i,j}}{y_{x^+ p}} + \frac{\exp(-\beta_i) \exp(-\beta_j) y_{r-1}^{\setminus i}y_{r-1}^{\setminus j}}{y_{x^+ p}^2}
\end{align}

Given these analytically derived formulas for the gradient and the Hessian, the item parameters can be updated using the Newton-Raphson algorithm. Convergence of the algorithm can be defined after a set number of iterations or once the change in $\beta$ between one iteration and the next is beneath a specified threshold. Here, both approaches can be combined. Namely, if the change in $\beta$ is negligeable between two iterations, the algorithm will stop but will otherwise continue until it reaches a maximum number of iterations. Both the threshold and maximum number of iterations are arbitrary.

### Sum Algorithm

The sum algorithm is defined recurrently:

* **Initialise Matrix**
* **Fill Matrix Using Recurrence Relation**
* **Return Probability of Observed Score**

```{r}
# Define the sum algorithm for the Rasch model
sum_algorithm_prob <- function(beta, score) {
  # Initialize a matrix with zeros, size (J+1) x (J+1), where J is the number of items
  results <- matrix(0, nrow = length(beta) + 1, ncol = length(beta) + 1) 
  
  # Set the first row to 1 (base case for score accumulation)
  results[1, ] <- 1
  
  # Calculate transformed item difficulties as exp(-beta_i)
  bi <- exp(-beta)
  
  # Fill in the results matrix using the recurrence relation
  for (i in 1:length(beta)) {
    for (k in 1:length(beta)) {
      results[k + 1, i + 1] <- bi[i] * results[k, i] + results[k + 1, i]
    }
  }
  
  # Return the probability for the given score
  return(results[score + 1, length(beta) + 1])  # +1 to handle score of 0
}
```

### Gradient for the Rasch Model

The formulation of the gradient vector is given in section 4.0.1:

\begin{equation}
\frac{\partial l}{\partial \beta_i} = - \sum_{p=1}^sx_{pi} + \frac{\exp(-\beta_i)y_{r-1}^{\setminus i}}{y_{x^+ p}}
\end{equation}

The steps to perform this in a computationally efficient manner are:

* **Initialise Variables**
* **Calculate Total Scores**
* **Initialise Gradient Vector**
* **Calculate Gradient for Each Item**
* **Update Gradient Vector**
* **Store and Return Gradient Vector**

```{r}
# Define the gradient (first derivative) of the log-likelihood using the sum algorithm
grad_rasch <- function(beta, response_matrix) {
  N <- nrow(response_matrix)  # Number of persons
  J <- ncol(response_matrix)  # Number of items
  
  S_i <- rowSums(response_matrix)  # Total correct responses per person
  gradient <- numeric(J)  # Initialize gradient vector of length J
  
  bi <- exp(-beta)  # Calculate exp(-beta) for each item
  
  # Calculate the gradient for each beta_j
  for (j in 1:J) {
    grad_sum <- 0  # Initialize sum for current beta_j
    
    for (i in 1:N) {
      observed_score <- S_i[i]  # Person's total score
      prob_score <- sum_algorithm_prob(beta, observed_score)  # Probability of observed score
      beta_without_j <- beta[-j]  # Exclude current beta for partial derivative
      prev_prob <- sum_algorithm_prob(beta_without_j, observed_score - 1)  # Previous probability
      
      # Update grad_sum only if observed_score > 0
      if (observed_score > 0) {
        grad_sum <- grad_sum + (-response_matrix[i, j]) + (bi[j] * prev_prob) / prob_score
      }
    }
    
    gradient[j] <- grad_sum  # Store gradient for beta_j
  }
  
  return(gradient)  # Return gradient vector
}
```
### Hessian for the Rasch Model

The formulation of the elements of the Hessian matrix is given in section 4.0.1. For the diagonal elements, these can be obtained with:

\begin{equation}
\frac{\partial^2 l}{\partial \beta_i^2} = \frac{-\exp(-\beta_i) y_{r-1}^{\setminus i}}{y_{x^+ p}} + \left( \frac{\exp(-\beta_i) y_{r-1}^{\setminus i}}{y_{x^+ p}} \right)^2
\end{equation}

For the off-diagonal elements, the formula is: 

\begin{equation}
\frac{\partial^2 l}{\partial \beta_i \partial \beta_j} = \frac{-\exp(-\beta_i) \exp(-\beta_j) y_{r-2}^{\setminus i,j}}{y_{x^+ p}} + \frac{\exp(-\beta_i) \exp(-\beta_j) y_{r-1}^{\setminus i}y_{r-1}^{\setminus j}}{y_{x^+ p}^2}
\end{equation}

The steps to obtain these elements in a computationally efficient manner are:

* **Initialise Variables**
* **Initialise Hessian Sum for Item Pairs**
* **Calculate Diagnonal Elements**
* **Calculate Off-Diagonal Elements**
* **Update Hessian Matrix**
* **Return Hessian Matrix**

```{r}
# Define the Hessian (second derivative) for the Newton-Raphson update using the sum algorithm
hessian_rasch <- function(beta, response_matrix) {
  N <- nrow(response_matrix)  # Number of persons
  J <- ncol(response_matrix)  # Number of items

  S_i <- rowSums(response_matrix)  # Total correct responses per person
  hessian <- matrix(0, J, J)  # Initialize Hessian matrix with zeros
  
  bi <- exp(-beta)  # Calculate exp(-beta) for each item
  
  # Compute the Hessian for each pair of beta_j and beta_k (cross-terms)
  for (j in 1:J) {
    for (k in 1:J) {
      hess_sum <- 0  # Initialize the sum for the Hessian entry
      
      for (i in 1:N) {
        observed_score <- S_i[i]  # Total score for person i
        prob_score <- sum_algorithm_prob(beta, observed_score)  # Probability of observed score

        # Diagonal elements: when j equals k
        if (j == k) {
          if (observed_score > 0) {  # Only calculate if the score allows for this derivative
            beta_without_j <- beta[-j]  # Exclude beta_j for the partial derivative
            prev_prob <- sum_algorithm_prob(beta_without_j, observed_score - 1)  # Probability for score - 1
            hess_sum <- hess_sum - (bi[j] * prev_prob) / prob_score + ((bi[j] * prev_prob) / prob_score)^2
          }
          
        # Off-diagonal elements: when j is not equal to k
        } else {
          if (observed_score > 1) {  # Only calculate if the score allows for this derivative
            beta_without_j_k <- beta[-c(j, k)]  # Exclude both beta_j and beta_k
            prev_prob_j_k <- sum_algorithm_prob(beta_without_j_k, observed_score - 2)  # Probability for score - 2
            prev_prob_j <- sum_algorithm_prob(beta[-j], observed_score - 1)  # Probability for score - 1 excluding beta_j
            prev_prob_k <- sum_algorithm_prob(beta[-k], observed_score - 1)  # Probability for score - 1 excluding beta_k
            hess_sum <- hess_sum - (bi[j] * bi[k] * prev_prob_j_k) / prob_score + (bi[j] * bi[k] * prev_prob_j * prev_prob_k) / prob_score^2
          }
        }
      }
      hessian[j, k] <- hess_sum  # Store the calculated value in the Hessian matrix
    }
  }
  
  return(hessian)  # Return the Hessian matrix
}
```
### Multivariate Newton-Raphson Algorithm for the Rasch Model

Given the previous functions for the sum algorithm, gradient vector and Hessian matrix, the Newton-Raphson Algorithm is defined as follows:

* **Define Inputs**
* **Initialise Variables**
* **Calculate Gradient and Hessian**
* **Update Beta**
* **Check for Convergence**
* **Return Estimated Beta Values**

```{r}
# Multivariate Newton-Raphson implementation
multivariate_newton_raphson <- function(beta_init, response_matrix, tol = 1e-6, max_iter = 1000, save_iterations = FALSE) {
  beta <- beta_init
  J <- length(beta_init)  # Number of items
  beta_history <- matrix(NA, nrow = max_iter, ncol = J)  # Matrix to store beta values per iteration
  
  for (iter in 1:max_iter) {
    print(iter)  # Track iteration progress
    
    # Calculate gradient and Hessian
    gradient <- grad_rasch(beta, response_matrix)
    hessian <- hessian_rasch(beta, response_matrix)
    
    # Update beta: beta_new = beta - H_inv * gradient
    beta_new <- beta - ginv(hessian) %*% gradient
    print(max(abs(beta_new - beta)))  # Display max change in beta for convergence check
    
    # Save current beta values if required
    if (save_iterations) {
      beta_history[iter, ] <- beta
    }
    
    # Check for convergence
    if (max(abs(beta_new - beta)) < tol) {
      cat("Converged after", iter, "iterations\n")
      # Trim beta_history to actual iterations if saving
      if (save_iterations) {
        beta_history <- beta_history[1:iter, ]
      }
      return(list(beta = beta_new, beta_history = beta_history))
    }
    
    # Update beta for next iteration
    beta <- beta_new
  }
  
  cat("Did not converge within the maximum number of iterations\n")
  return(list(beta = beta, beta_history = beta_history))
}
```

## Application of the Newton-Raphson Algorithm

Initial values for the Newton-algorithm must be defined before using it on the data generated earlier. We choose a vector of zeroes so as to obtain difficulties centered around 0. This is consistent with the data generation process, during which the item difficulties were first centered. An equivalent alternative would be to choose different starting values then center the estimated item difficulties once the algorithm has converged.

```{r}
# Initial guess for beta
beta_init <- rep(0, ncol(response_matrix))

# Run the multivariate Newton-Raphson algorithm
beta_output <- multivariate_newton_raphson(beta_init, response_matrix, save_iterations = TRUE)

# Print the final estimated beta values
beta_estimated <- beta_output$beta
print(beta_estimated)

# Compare the estimated beta with the true beta
comparison <- data.frame(
  Item = 1:length(beta_true),
  True_Beta = beta_true,
  Estimated_Beta = beta_estimated
)

# Display the comparison
print(comparison)
```


The comparison given above indicates that the algorithm is succesful as similar values to those used for the data generation are found. We can also use the rasch function from the ltm package to get item difficulty parameters:


```{r}
rasch_model <- rasch(response_matrix)
beta_estimates_ltm <- coef(rasch_model)
beta_estimates_ltm
```

Again, there is little difference between our estimated parameters via the rasch function and the true parameters. This also confirms that retrieving the original parameters perfectly is not possible given error in the response matrix. Finally, we can verify that the Newton-Raphson function is converging properly and that the values observed are not due to chance or convergence in a local maximum. This can be done easily as the function already stores the values of $\beta$ at each iteration:

```{r}
# Since we already saved the beta history, we can plot it:
beta_history <- beta_output$beta_history

# Convert beta_history to a data frame for plotting
beta_history_df <- as.data.frame(beta_history)
beta_history_df$Iteration <- 1:nrow(beta_history_df)
beta_history_long <- reshape2::melt(beta_history_df, id.vars = "Iteration", variable.name = "Item", value.name = "Beta")

# Rename the Item variable for clearer legend labels
beta_history_long$Item <- factor(beta_history_long$Item, labels = paste0("Item ", 1:ncol(beta_history)))

# Plot convergence of each beta value with customized legend labels
ggplot(beta_history_long, aes(x = Iteration, y = Beta, color = Item)) +
  geom_line() +
  labs(title = "Convergence of beta values across iterations", x = "Iteration", y = "Beta Estimate", color = "Item") +
  theme_minimal()
```

Convergence is obvious 

# Evaluation of the Function's Correctness

In order to evaluate, whether the function correctly estimates the item parameters, we can compare the estimated item parameters with the true item parameters in various scenarios, letting the number of items and persons, as well as the item difficulties and their distribution vary. The following table presents the conditions under which the functions were evaluated:

```{r}
table <- data.frame(matrix(ncol = 1, nrow = 3))

rownames(table) <- c("Number of Persons", "Number of Items", "Item Difficulty Distribution")

table[1,] <- c("100, 1000, 10000")
table[2,] <- c("3, 5, 10")
table[3,] <- c("Uniform, Normal, Exponential")

kable(table, col.names = "Conditions", caption = "Conditions for the evaluation of the functions correctness") %>%
  kable_styling(full_width = F)
```

Given that there are three different conditions with three different scenarios each, we can evaluate the functions correctness in a total of 27 scenarios. Below these scenarios are presented:

```{r}
person_conditions <- c(100, 1000, 10000)
item_conditions <- c(3, 5, 10)
item_difficulty_distributions <- c("uniform", "normal", "exponential")

# Create MAE_array
MAE_array_own <- array(NA, dim = c(length(person_conditions), length(item_conditions), length(item_difficulty_distributions)))
dimnames(MAE_array_own) <- list(person_conditions, item_conditions, item_difficulty_distributions) # label the dimensions of the array

MAE_array_ltm <- array(NA, dim = c(length(person_conditions), length(item_conditions), length(item_difficulty_distributions)))
dimnames(MAE_array_ltm) <- list(person_conditions, item_conditions, item_difficulty_distributions) # label the dimensions of the array

for (i in seq_along(person_conditions)) {
  for (j in seq_along(item_conditions)) {
    for (k in seq_along(item_difficulty_distributions)) {
      print(paste("Scenario:", i, j, k))
      
      #### Simulate Responses and Estimate Item Parameters for Each Scenario ####

      # Step 1: Define Item Parameters
      set.seed(124) # for reproducibility
      num_items <- item_conditions[i] # number of items
      beta_true <- if (item_difficulty_distributions[j] == "uniform") {
        runif(num_items, -2, 2) # uniformly distributed item difficulties around 0 logits
      } else if (item_difficulty_distributions[j] == "normal") {
        rnorm(num_items, 0, 1) # normally distributed item difficulties with mean 0 and SD 1
      } else if (item_difficulty_distributions[j] == "exponential"){
        rexp(num_items, 1) # exponentially distributed item difficulties with rate 1
      } else {
        stop("Invalid item difficulty distribution")
      }
      
      beta_true <- scale(beta_true, scale = F) # Center the betas because they are relative
      
      # Step 2: Define Person Parameters
      num_persons <- person_conditions[k] # sample size
      person_ability <- rnorm(num_persons, -1, 1) # normally distributed person abilities with mean 0 and SD 1
      person_ability <- scale(person_ability, scale = F) # Center the thetas because they are relative
      
      # Step 4: Simulate Responses
      # Create an empty response matrix to store responses for each person-item pair
      response_matrix <- matrix(0, nrow = num_persons, ncol = num_items)
      
      for (l in 1:num_persons) {
        for (m in 1:num_items) {
          # Step 4A: Generate a random number U from uniform [0,1]
          U <- runif(1)
          
          # Step 4B: Compute probability of failure
          prob_failure <- 1 / (1 + exp(person_ability[l] - beta_true[m]))
          
          # Step 4C: Check if U > Probability of failure
          if (U > prob_failure) {
            response_matrix[l, m] <- 1 # Correct response
          } else {
            response_matrix[l, m] <- 0 # Incorrect response
          }
        }
      }

      #### Estimate Item Parameters Using the Multivariate Newton-Raphson Algorithm ####
      beta_init <- rep(0, ncol(response_matrix)) # Initial value for beta
      beta_estimated <- multivariate_newton_raphson(beta_init, response_matrix, save_iterations = TRUE)$beta
      
      #### Compute the Mean Absolute Error (MAE) between the True and Estimated Item Parameters ####
      MAE_own <- mean(abs(beta_true - beta_estimated)) # own function
      MAE_ltm <- mean(abs(beta_true - coef(rasch(response_matrix))[,1])) # ltm function
      
      # save MAE in 3 x 3 x 3 array
      MAE_array_own[i, j, k] <- MAE_own
      MAE_array_ltm[i, j, k] <- MAE_ltm
    }
  }
}


```

The Mean Absolute Error (MAE) between the true and estimated item parameters for each scenario is presented dividing the scenarios into three tables, one for each item difficulty distribution:

```{r}
for (i in seq_along(item_difficulty_distributions)) {
  # Convert matrix to data frame and add row and column names
  df <- as.data.frame(MAE_array_own[,,i])
  df$Persons <- rownames(df) # Add row names as a column
  df <- melt(df, id.vars = "Persons", variable.name = "Items", value.name = "MAE")
  
  # Ensure Persons and Items are factors for better control over discrete axes
  df$Persons <- factor(df$Persons)
  df$Items <- factor(df$Items)
  
  # Generate the heatmap with annotations
  p <- ggplot(df, aes(x = Persons, y = Items, fill = MAE)) +
    geom_tile(color = "white") + # White borders for clarity
    geom_text(aes(label = round(MAE, 3)), color = "black", size = 3) + # Add MAE values to each tile
    scale_fill_gradient(low = "green", high = "red", limits = c(0, 0.5)) +
    labs(title = paste0("Mean Absolute Error of Estimated Item Parameters (", item_difficulty_distributions[i], ")"),
         x = "Number of Persons", y = "Number of Items") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels for readability
  
  # Print the plot explicitly
  print(p)
}

for (i in seq_along(item_difficulty_distributions)) {
  # Convert matrix to data frame and add row and column names
  df <- as.data.frame(MAE_array_ltm[,,i])
  df$Persons <- rownames(df) # Add row names as a column
  df <- melt(df, id.vars = "Persons", variable.name = "Items", value.name = "MAE")
  
  # Ensure Persons and Items are factors for better control over discrete axes
  df$Persons <- factor(df$Persons)
  df$Items <- factor(df$Items)
  
  # Generate the heatmap with annotations
  p <- ggplot(df, aes(x = Persons, y = Items, fill = MAE)) +
    geom_tile(color = "white") + # White borders for clarity
    geom_text(aes(label = round(MAE, 3)), color = "black", size = 3) + # Add MAE values to each tile
    scale_fill_gradient(low = "green", high = "red", limits = c(0, 0.5)) +
    labs(title = paste0("Mean Absolute Error of Estimated Item Parameters (", item_difficulty_distributions[i], ")"),
         x = "Number of Persons", y = "Number of Items") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels for readability
  
  # Print the plot explicitly
  print(p)
}
```

As can be seen in the plots, both the implemented and the `ltm` functions perform similarly in estimating the item parameters. The Mean Absolute Error (MAE) between the true and estimated item parameters is generally low with normal and exponentially distributed true item parameters. In the cases of these distributions, the MAEs are generally around 0.05 implying that the estimates on average only deviate by 0.05 from the true values. This indicates that the implemented functions are quite reliable for estimating item parameters in the Rasch model given exponentially or normally distributed parameters.
In the case of uniformly distributed true item parameters, the MAEs are generally higher, with values going up to 0.223 in the implemented function and even 0.332 for the `rasch` function in the `ltm` page. This indicates that the implemented and built-in functions are less reliable for estimating item parameters in the Rasch model given uniformly distributed parameters.

