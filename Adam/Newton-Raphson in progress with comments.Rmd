---
title: "Rasch-model"
author: "Merlin"
date: "2024-10-09"
output:
  html_document: default
  pdf_document: default
---

# Load Packages

```{r}
# Required package for matrix operations
library(MASS)  # For the ginv() function (generalized inverse)
```

# The data simulation

```{r}
generate_irt_data <- function(num_items = 3, num_persons = 1000, seed = 124) {
  
  set.seed(seed) # Set seed for reproducibility
  
  # Step 1: Define Item Parameters
  beta_true <- runif(num_items, -2, 2) # uniformly distributed item difficulties around 0 logits
  beta_true <- scale(beta_true, scale = FALSE) # Center the betas because they are relative
  
  # Step 2: Define Person Parameters with fixed mean and SD
  person_ability <- rnorm(num_persons, mean = -1, sd = 1) # normally distributed person abilities
  person_ability <- scale(person_ability, scale = FALSE) # Center the thetas because they are relative
  
  # Step 4: Simulate Responses
  # Create an empty response matrix to store responses for each person-item pair
  response_matrix <- matrix(0, nrow = num_persons, ncol = num_items)
  
  for (i in 1:num_persons) {
    for (j in 1:num_items) {
      # Step 4A: Generate a random number U from uniform [0,1]
      U <- runif(1)
      
      # Step 4B: Compute probability of failure
      prob_failure <- 1 / (1 + exp(person_ability[i] - beta_true[j]))
      
      # Step 4C: Check if U > Probability of failure
      if (U > prob_failure) {
        response_matrix[i, j] <- 1 # Correct response
      } else {
        response_matrix[i, j] <- 0 # Incorrect response
      }
    }
  }
  
  # Return both the response matrix and the true beta values
  return(list(response_matrix = response_matrix, beta_true = beta_true))
}
```

## Generate Example Data
```{r}
# Generate example data with 5 items and 1000 persons
result <- generate_irt_data(num_items = 5, num_persons = 1000)

response_matrix <- result$response_matrix
beta_true <- result$beta_true
```

# The Estimation

### Sum Algorithm for the Rasch Model

* **Initialize Matrix**
* **Compute Item Difficulty Transformation**
* **Fill Matrix Using Recurrence Relation**
* **Return Probability of Observed Score**

```{r}
# Define the sum algorithm for the Rasch model
sum_algorithm_prob <- function(beta, score) {
  # Initialize a matrix with zeros, size (J+1) x (J+1), where J is the number of items
  results <- matrix(0, nrow = length(beta) + 1, ncol = length(beta) + 1) 
  
  # Set the first row to 1 (base case for score accumulation)
  results[1, ] <- 1
  
  # Calculate transformed item difficulties as exp(-beta_i)
  bi <- exp(-beta)
  
  # Fill in the results matrix using the recurrence relation
  for (i in 1:length(beta)) {
    for (k in 1:length(beta)) {
      results[k + 1, i + 1] <- bi[i] * results[k, i] + results[k + 1, i]
    }
  }
  
  # Return the probability for the given score
  return(results[score + 1, length(beta) + 1])  # +1 to handle score of 0
}
```

### Gradient for the Rasch Model
* **Define Inputs**
* **Initialise Variables**
* **Calculate Total Scores**
* **Initialise Gradient Vector**
* **Calculate Gradient for Each Item**
* **Update Gradient Vector**
* **Store and Return Gradient Vector**

```{r}
# Define the gradient (first derivative) of the log-likelihood using the sum algorithm
grad_rasch <- function(beta, response_matrix) {
  N <- nrow(response_matrix)  # Number of persons
  J <- ncol(response_matrix)  # Number of items
  
  S_i <- rowSums(response_matrix)  # Total correct responses per person
  gradient <- numeric(J)  # Initialize gradient vector of length J
  
  bi <- exp(-beta)  # Calculate exp(-beta) for each item
  
  # Calculate the gradient for each beta_j
  for (j in 1:J) {
    grad_sum <- 0  # Initialize sum for current beta_j
    
    for (i in 1:N) {
      observed_score <- S_i[i]  # Person's total score
      prob_score <- sum_algorithm_prob(beta, observed_score)  # Probability of observed score
      beta_without_j <- beta[-j]  # Exclude current beta for partial derivative
      prev_prob <- sum_algorithm_prob(beta_without_j, observed_score - 1)  # Previous probability
      
      # Update grad_sum only if observed_score > 0
      if (observed_score > 0) {
        grad_sum <- grad_sum + (-response_matrix[i, j]) + (bi[j] * prev_prob) / prob_score
      }
    }
    
    gradient[j] <- grad_sum  # Store gradient for beta_j
  }
  
  return(gradient)  # Return gradient vector
}
```
### Hessian for the Rasch Model
* **Define Inputs**
* **Initialise Variables**
* **Initialise Hessian Sum for Item Pairs**
* **Calculate Diagnonal Elements**
* **Calculate Off-Diagonal Elements**
* **Update Hessian Matrix**
* **Return Hessian Matrix**
```{r}
# Define the Hessian (second derivative) for the Newton-Raphson update using the sum algorithm
hessian_rasch <- function(beta, response_matrix) {
  N <- nrow(response_matrix)  # Number of persons
  J <- ncol(response_matrix)  # Number of items

  S_i <- rowSums(response_matrix)  # Total correct responses per person
  hessian <- matrix(0, J, J)  # Initialize Hessian matrix with zeros
  
  bi <- exp(-beta)  # Calculate exp(-beta) for each item
  
  # Compute the Hessian for each pair of beta_j and beta_k (cross-terms)
  for (j in 1:J) {
    for (k in 1:J) {
      hess_sum <- 0  # Initialize the sum for the Hessian entry
      
      for (i in 1:N) {
        observed_score <- S_i[i]  # Total score for person i
        prob_score <- sum_algorithm_prob(beta, observed_score)  # Probability of observed score

        # Diagonal elements: when j equals k
        if (j == k) {
          if (observed_score > 0) {  # Only calculate if the score allows for this derivative
            beta_without_j <- beta[-j]  # Exclude beta_j for the partial derivative
            prev_prob <- sum_algorithm_prob(beta_without_j, observed_score - 1)  # Probability for score - 1
            hess_sum <- hess_sum - (bi[j] * prev_prob) / prob_score + ((bi[j] * prev_prob) / prob_score)^2
          }
          
        # Off-diagonal elements: when j is not equal to k
        } else {
          if (observed_score > 1) {  # Only calculate if the score allows for this derivative
            beta_without_j_k <- beta[-c(j, k)]  # Exclude both beta_j and beta_k
            prev_prob_j_k <- sum_algorithm_prob(beta_without_j_k, observed_score - 2)  # Probability for score - 2
            prev_prob_j <- sum_algorithm_prob(beta[-j], observed_score - 1)  # Probability for score - 1 excluding beta_j
            prev_prob_k <- sum_algorithm_prob(beta[-k], observed_score - 1)  # Probability for score - 1 excluding beta_k
            hess_sum <- hess_sum - (bi[j] * bi[k] * prev_prob_j_k) / prob_score + (bi[j] * bi[k] * prev_prob_j * prev_prob_k) / prob_score^2
          }
        }
      }
      hessian[j, k] <- hess_sum  # Store the calculated value in the Hessian matrix
    }
  }
  
  return(hessian)  # Return the Hessian matrix
}
```
### Multivariate Newton-Raphson Algorithm for the Rasch Model

* **Define Inputs**
* **Initialise Variables**
* **Calculate Gradient and Hessian**
* **Update Beta**
* **Check for Convergence**
* **Return Estimated Beta Values**
```{r}
# Multivariate Newton-Raphson implementation
multivariate_newton_raphson <- function(beta_init, response_matrix, tol = 1e-6, max_iter = 1000, save_iterations = FALSE) {
  beta <- beta_init
  J <- length(beta_init)  # Number of items
  beta_history <- matrix(NA, nrow = max_iter, ncol = J)  # Matrix to store beta values per iteration
  
  for (iter in 1:max_iter) {
    print(iter)  # Track iteration progress
    
    # Calculate gradient and Hessian
    gradient <- grad_rasch(beta, response_matrix)
    hessian <- hessian_rasch(beta, response_matrix)
    
    # Update beta: beta_new = beta - H_inv * gradient
    beta_new <- beta - ginv(hessian) %*% gradient
    print(max(abs(beta_new - beta)))  # Display max change in beta for convergence check
    
    # Save current beta values if required
    if (save_iterations) {
      beta_history[iter, ] <- beta
    }
    
    # Check for convergence
    if (max(abs(beta_new - beta)) < tol) {
      cat("Converged after", iter, "iterations\n")
      # Trim beta_history to actual iterations if saving
      if (save_iterations) {
        beta_history <- beta_history[1:iter, ]
      }
      return(list(beta = beta_new, beta_history = beta_history))
    }
    
    # Update beta for next iteration
    beta <- beta_new
  }
  
  cat("Did not converge within the maximum number of iterations\n")
  return(list(beta = beta, beta_history = beta_history))
}
```

## Application of the Newton-Raphson Algorithm

```{r}
# Initial guess for beta
beta_init <- rep(0, ncol(response_matrix))

# Run the multivariate Newton-Raphson algorithm
beta_output <- multivariate_newton_raphson(beta_init, response_matrix, save_iterations = TRUE)

# Print the final estimated beta values
beta_estimated <- beta_output$beta
print(beta_estimated)

# Compare the estimated beta with the true beta
comparison <- data.frame(
  Item = 1:length(beta_true),
  True_Beta = beta_true,
  Estimated_Beta = beta_estimated
)

# Display the comparison
print(comparison)
```

The comparison with the rasch model

```{r}
library(ltm)
rasch_model <- rasch(response_matrix)
beta_estimates_ltm <- coef(rasch_model)
summary(rasch_model)
beta_estimates_ltm
```

Graph that shows convergence:

```{r}
library(ggplot2) # For plotting
library(reshape2) # For melt

# Since we already saved the beta history, we can plot it:
beta_history <- beta_output$beta_history

# Convert beta_history to a data frame for plotting
beta_history_df <- as.data.frame(beta_history)
beta_history_df$Iteration <- 1:nrow(beta_history_df)
beta_history_long <- reshape2::melt(beta_history_df, id.vars = "Iteration", variable.name = "Item", value.name = "Beta")

# Rename the Item variable for clearer legend labels
beta_history_long$Item <- factor(beta_history_long$Item, labels = paste0("Item ", 1:ncol(beta_history)))

# Plot convergence of each beta value with customized legend labels
ggplot(beta_history_long, aes(x = Iteration, y = Beta, color = Item)) +
  geom_line() +
  labs(title = "Convergence of beta values across iterations", x = "Iteration", y = "Beta Estimate", color = "Item") +
  theme_minimal()
```

# Evaluation of the Correctness of the Generated Data



# Evaluation of the Functions Correctness

In order to evaluate, whether the function correctly estimates the item parameters, we can compare the estimated item parameters with the true item parameters in various scenarios, letting the number of items and persons, as well as the item difficulties and their distribution vary. The following table presents the conditions under which the functions were evaluated:

```{r}
library(knitr)
library(kableExtra)
table <- data.frame(matrix(ncol = 1, nrow = 3))

rownames(table) <- c("Number of Persons", "Number of Items", "Item Difficulty Distribution")

table[1,] <- c("100, 1000, 10000")
table[2,] <- c("3, 5, 10")
table[3,] <- c("Uniform, Normal, Exponential")

kable(table, col.names = "Conditions", caption = "Conditions for the evaluation of the functions correctness") %>%
  kable_styling(full_width = F)
```

Given that there are three different conditions with three different scenarios each, we can evaluate the functions correctness in a total of 27 scenarios. Below these scenarios are presented:

```{r}
person_conditions <- c(100, 1000, 10000)
item_conditions <- c(3, 5, 10)
item_difficulty_distributions <- c("uniform", "normal", "exponential")

# Create MAE_array
MAE_array_own <- array(NA, dim = c(length(person_conditions), length(item_conditions), length(item_difficulty_distributions)))
dimnames(MAE_array_own) <- list(person_conditions, item_conditions, item_difficulty_distributions) # label the dimensions of the array

MAE_array_ltm <- array(NA, dim = c(length(person_conditions), length(item_conditions), length(item_difficulty_distributions)))
dimnames(MAE_array_ltm) <- list(person_conditions, item_conditions, item_difficulty_distributions) # label the dimensions of the array

for (i in seq_along(person_conditions)) {
  for (j in seq_along(item_conditions)) {
    for (k in seq_along(item_difficulty_distributions)) {
      print(paste("Scenario:", i, j, k))
      
      #### Simulate Responses and Estimate Item Parameters for Each Scenario ####

      # Step 1: Define Item Parameters
      set.seed(124) # for reproducibility
      num_items <- item_conditions[i] # number of items
      beta_true <- if (item_difficulty_distributions[j] == "uniform") {
        runif(num_items, -2, 2) # uniformly distributed item difficulties around 0 logits
      } else if (item_difficulty_distributions[j] == "normal") {
        rnorm(num_items, 0, 1) # normally distributed item difficulties with mean 0 and SD 1
      } else if (item_difficulty_distributions[j] == "exponential"){
        rexp(num_items, 1) # exponentially distributed item difficulties with rate 1
      } else {
        stop("Invalid item difficulty distribution")
      }
      
      beta_true <- scale(beta_true, scale = F) # Center the betas because they are relative
      
      # Step 2: Define Person Parameters
      num_persons <- person_conditions[k] # sample size
      person_ability <- rnorm(num_persons, -1, 1) # normally distributed person abilities with mean 0 and SD 1
      person_ability <- scale(person_ability, scale = F) # Center the thetas because they are relative
      
      # Step 4: Simulate Responses
      # Create an empty response matrix to store responses for each person-item pair
      response_matrix <- matrix(0, nrow = num_persons, ncol = num_items)
      
      for (l in 1:num_persons) {
        for (m in 1:num_items) {
          # Step 4A: Generate a random number U from uniform [0,1]
          U <- runif(1)
          
          # Step 4B: Compute probability of failure
          prob_failure <- 1 / (1 + exp(person_ability[l] - beta_true[m]))
          
          # Step 4C: Check if U > Probability of failure
          if (U > prob_failure) {
            response_matrix[l, m] <- 1 # Correct response
          } else {
            response_matrix[l, m] <- 0 # Incorrect response
          }
        }
      }

      #### Estimate Item Parameters Using the Multivariate Newton-Raphson Algorithm ####
      beta_init <- rep(0, ncol(response_matrix)) # Initial value for beta
      beta_estimated <- multivariate_newton_raphson(beta_init, response_matrix, save_iterations = TRUE)$beta
      
      #### Compute the Mean Absolute Error (MAE) between the True and Estimated Item Parameters ####
      MAE_own <- mean(abs(beta_true - beta_estimated)) # own function
      MAE_ltm <- mean(abs(beta_true - coef(rasch(response_matrix))[,1])) # ltm function
      
      # save MAE in 3 x 3 x 3 array
      MAE_array_own[i, j, k] <- MAE_own
      MAE_array_ltm[i, j, k] <- MAE_ltm
    }
  }
}


```

The Mean Absolute Error (MAE) between the true and estimated item parameters for each scenario is presented dividing the scenarios into three tables, one for each item difficulty distribution:

```{r}
# Load necessary libraries
library(ggplot2)
library(reshape2)


for (i in seq_along(item_difficulty_distributions)) {
  # Convert matrix to data frame and add row and column names
  df <- as.data.frame(MAE_array_own[,,i])
  df$Persons <- rownames(df) # Add row names as a column
  df <- melt(df, id.vars = "Persons", variable.name = "Items", value.name = "MAE")
  
  # Ensure Persons and Items are factors for better control over discrete axes
  df$Persons <- factor(df$Persons)
  df$Items <- factor(df$Items)
  
  # Generate the heatmap with annotations
  p <- ggplot(df, aes(x = Persons, y = Items, fill = MAE)) +
    geom_tile(color = "white") + # White borders for clarity
    geom_text(aes(label = round(MAE, 3)), color = "black", size = 3) + # Add MAE values to each tile
    scale_fill_gradient(low = "green", high = "red", limits = c(0, 0.5)) +
    labs(title = paste0("Mean Absolute Error of Estimated Item Parameters (", item_difficulty_distributions[i], ")"),
         x = "Number of Persons", y = "Number of Items") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels for readability
  
  # Print the plot explicitly
  print(p)
}

for (i in seq_along(item_difficulty_distributions)) {
  # Convert matrix to data frame and add row and column names
  df <- as.data.frame(MAE_array_ltm[,,i])
  df$Persons <- rownames(df) # Add row names as a column
  df <- melt(df, id.vars = "Persons", variable.name = "Items", value.name = "MAE")
  
  # Ensure Persons and Items are factors for better control over discrete axes
  df$Persons <- factor(df$Persons)
  df$Items <- factor(df$Items)
  
  # Generate the heatmap with annotations
  p <- ggplot(df, aes(x = Persons, y = Items, fill = MAE)) +
    geom_tile(color = "white") + # White borders for clarity
    geom_text(aes(label = round(MAE, 3)), color = "black", size = 3) + # Add MAE values to each tile
    scale_fill_gradient(low = "green", high = "red", limits = c(0, 0.5)) +
    labs(title = paste0("Mean Absolute Error of Estimated Item Parameters (", item_difficulty_distributions[i], ")"),
         x = "Number of Persons", y = "Number of Items") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels for readability
  
  # Print the plot explicitly
  print(p)
}
```

As can be seen in the plots, both the implemented and the `ltm` functions perform similarly in estimating the item parameters. The Mean Absolute Error (MAE) between the true and estimated item parameters is generally low with normal and exponentially distributed true item parameters. In the cases of these distributions, the MAEs are generally around 0.05 implying that the estimates on average only deviate by 0.05 from the true values. This indicates that the implemented functions quite reliable for estimating item parameters in the Rasch model given exponentially or normally distributed parameters.
In the case of uniformly distributed true item parameters, the MAEs are generally higher, with values going up to 0.223 in the implemented function and even 0.332 for the `rasch` function in the `ltm` page. This indicates that the implemented functions are less reliable for estimating item parameters in the Rasch model given uniformly distributed parameters.

